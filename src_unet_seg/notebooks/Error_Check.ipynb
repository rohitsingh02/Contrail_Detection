{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041ac14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "# !pip install --upgrade segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e5af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import gc\n",
    "import pandas as pd\n",
    "import ttach as tta\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import cv2\n",
    "from torch.cuda import amp\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torchvision.utils import make_grid\n",
    "import optuna\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157ff8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e6bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb77d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"train\"  # \"validation\"\n",
    "    \n",
    "NTB = 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5654e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ddaafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58a5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "class ContrailDataset:\n",
    "    def __init__(self, df, transform=None, normalize=False):\n",
    "        self.df = df  \n",
    "        self.images = df['image']\n",
    "        self.labels = df['label']\n",
    "        self.transform =transform\n",
    "        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        self.normalize=normalize\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(\"../../input/\" + self.images[idx]).astype(float)   \n",
    "        label = np.load(\"../../input/\" + self.labels[idx]).astype(float)\n",
    "        \n",
    "\n",
    "        if self.transform :\n",
    "            data = self.transform(image=image, mask=label)\n",
    "            image  = data['image']\n",
    "            label  = data['mask']\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            label = np.transpose(label, (2, 0, 1))    \n",
    "            \n",
    "            \n",
    "#         return torch.tensor(image), torch.tensor(label)\n",
    "    \n",
    "        if self.normalize:\n",
    "            image = self.normalize_image(torch.tensor(image))\n",
    "            return image\n",
    "        else:\n",
    "            return torch.tensor(image), torch.tensor(label)\n",
    "    \n",
    "    \n",
    "# class ContrailDataset:\n",
    "#     def __init__(self, df, transform=None):\n",
    "#         self.df = df  \n",
    "#         self.images = df['image']\n",
    "#         self.transform =transform\n",
    "        \n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         image = np.load(\"../../input/\" + self.images[idx]).astype(float)   \n",
    "# #         label = np.load(\"../../input\" + self.labels[idx]).astype(float)\n",
    "        \n",
    "        \n",
    "#         # label_cls = 1 if label.sum() > 0 else 0\n",
    "#         if self.transform :\n",
    "#             data = self.transform(image=image)\n",
    "#             image  = data['image']\n",
    "#             image = np.transpose(image, (2, 0, 1))\n",
    "            \n",
    "#         return torch.tensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "985b3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TImmunetplusplus model Nirjhar\n",
    "\n",
    "n_blocks = 4\n",
    "\n",
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, cfg, segtype='unet', pretrained=True):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(12, 36, 3, stride=1, padding=1, bias=False)\n",
    "        self.mybn1 = nn.BatchNorm2d(6)\n",
    "        self.mybn2 = nn.BatchNorm2d(12)\n",
    "        self.mybn3 = nn.BatchNorm2d(36)     \n",
    "        self.encoder = timm.create_model(\n",
    "            cfg['backbone'],\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=0.8,\n",
    "            drop_path_rate=0.5,\n",
    "            pretrained=False\n",
    "        )\n",
    "        self.encoder.conv_stem=nn.Conv2d(6, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        self.encoder.blocks[5] = nn.Identity()\n",
    "        self.encoder.blocks[6] = nn.Sequential(\n",
    "            nn.Conv2d(self.encoder.blocks[4][2].conv_pwl.out_channels, 320, 1),\n",
    "            nn.BatchNorm2d(320),\n",
    "            nn.ReLU6(),\n",
    "        )\n",
    "        tr = torch.randn(1,6,64,64)\n",
    "        g = self.encoder(tr)\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.decoders.unetplusplus.decoder.UnetPlusPlusDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Conv2d(\n",
    "            decoder_channels[n_blocks-1],\n",
    "            cfg[\"num_classes\"], \n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1), \n",
    "            padding=(1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu6(self.mybn1(self.conv1(x)))\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "    \n",
    "\n",
    "def load_model1(cfg):\n",
    "    model = TimmSegModel(cfg)\n",
    "    model.load_state_dict(torch.load(cfg['model_pth']))\n",
    "    return model\n",
    "\n",
    "\n",
    "#### Model 2 Nirjhar\n",
    "\n",
    "n_blocks =4\n",
    "class TimmSegModel2(nn.Module):\n",
    "    def __init__(self, cfg, segtype='unet', pretrained=True):\n",
    "        super(TimmSegModel2, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            cfg['backbone'],\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=0.5,\n",
    "            pretrained=False\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 128, 128))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.decoders.unetplusplus.decoder.UnetPlusPlusDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv2d(decoder_channels[n_blocks-1], 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "    \n",
    "\n",
    "def load_model2(cfg):\n",
    "    model = TimmSegModel2(cfg)\n",
    "    model.load_state_dict(torch.load(cfg['model_pth']))\n",
    "    return model\n",
    "\n",
    "\n",
    "##################\n",
    "\n",
    "n_blocks = 4\n",
    "\n",
    "class TimmSegModel3(nn.Module):\n",
    "    def __init__(self, cfg, segtype='unet', pretrained=True):\n",
    "        super(TimmSegModel3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(12, 36, 3, stride=1, padding=1, bias=False)\n",
    "        self.mybn1 = nn.BatchNorm2d(6)\n",
    "        self.mybn2 = nn.BatchNorm2d(12)\n",
    "        self.mybn3 = nn.BatchNorm2d(36)     \n",
    "        self.encoder = timm.create_model(\n",
    "            cfg[\"backbone\"],\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=0.8,\n",
    "            drop_path_rate=0.5,\n",
    "            pretrained=False\n",
    "        )\n",
    "        self.encoder.conv_stem=nn.Conv2d(6, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        self.encoder.blocks[5] = nn.Identity()\n",
    "        self.encoder.blocks[6] = nn.Sequential(\n",
    "            nn.Conv2d(self.encoder.blocks[4][2].conv_pwl.out_channels, 320, 1),\n",
    "            nn.BatchNorm2d(320),\n",
    "            nn.ReLU6(),\n",
    "        )\n",
    "        tr = torch.randn(1,6,64,64)\n",
    "        g = self.encoder(tr)\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.decoders.unetplusplus.decoder.UnetPlusPlusDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], cfg['num_classes'], kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu6(self.mybn1(self.conv1(x)))\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "    \n",
    "    \n",
    "    \n",
    "def load_model3(cfg):\n",
    "    model = TimmSegModel3(cfg)\n",
    "    model.load_state_dict(torch.load(cfg['model_pth']))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class TimmSegModel4(nn.Module):\n",
    "    def __init__(self, cfg, segtype='unet', pretrained=True):\n",
    "        super(TimmSegModel4, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            cfg[\"backbone\"],\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=0.5,\n",
    "            pretrained=False\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 512, 512))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.decoders.unetplusplus.decoder.UnetPlusPlusDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv2d(decoder_channels[n_blocks-1], 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "    \n",
    "    \n",
    "def load_model4(cfg):\n",
    "    model = TimmSegModel4(cfg)\n",
    "    model.load_state_dict(torch.load(cfg['model_pth']))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Rohit Model \n",
    "class Net_R(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=cfg[\"backbone\"],     \n",
    "            encoder_weights=None,   \n",
    "            in_channels=3,  \n",
    "            classes=cfg[\"num_classes\"],\n",
    "            activation=None\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        mask = self.model(inputs)\n",
    "        return mask\n",
    "    \n",
    "    \n",
    "def load_modelr1(cfg):\n",
    "    model = Net_R(cfg)\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    model.load_state_dict(torch.load(cfg['model_pth'], map_location=torch.device('cpu'))['model'])\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class TimmSegModelR1(nn.Module):\n",
    "    def __init__(self, cfg, segtype='unet', pretrained=True):\n",
    "        super(TimmSegModelR1, self).__init__()\n",
    "\n",
    "        self.n_blocks = 4\n",
    "        self.encoder = timm.create_model(\n",
    "            cfg['backbone'],\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=0.5,\n",
    "            pretrained=False\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 128, 128))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.decoders.unetplusplus.decoder.UnetPlusPlusDecoder(\n",
    "                encoder_channels=encoder_channels[:self.n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:self.n_blocks],\n",
    "                n_blocks=self.n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv2d(decoder_channels[self.n_blocks-1], \n",
    "                      cfg['num_classes'],\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=(1, 1)\n",
    "            ),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = [0] + self.encoder(x)[:self.n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "\n",
    "\n",
    "    \n",
    "def load_modelr2(cfg):\n",
    "    model = TimmSegModelR1(cfg)\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    model.load_state_dict(torch.load(cfg['model_pth'], map_location=torch.device('cpu'))['model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35da6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07bfce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, thr=0.5, epsilon=1e-6):\n",
    "    y_true = y_true.to(torch.float32)\n",
    "    y_pred = (y_pred>thr).to(torch.float32)\n",
    "    inter = (y_true*y_pred).sum()\n",
    "    den = y_true.sum() + y_pred.sum()\n",
    "    dice = ((2*inter+epsilon)/(den+epsilon)).mean()\n",
    "    \n",
    "    return dice\n",
    "\n",
    "\n",
    "def get_transform(img_size):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(*img_size, interpolation=cv2.INTER_NEAREST),\n",
    "    ], p=1.0)\n",
    "    return transform\n",
    "\n",
    "def get_transform2(img_size):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(*img_size, interpolation=cv2.INTER_LINEAR),\n",
    "    ], p=1.0)\n",
    "    return transform\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01378f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df = pd.read_csv(\"../../input/data_utils/val_df_filled.csv\")\n",
    "# val_df_full = val_df.copy()\n",
    "# val_dups = np.load(\"../../input/data_utils/dups_val.npy\")\n",
    "# val_dups = [int(val_id) for val_id in val_dups]\n",
    "\n",
    "# val_df = val_df.loc[~val_df['id'].isin(val_dups)].reset_index(drop=True)\n",
    "# print(val_df.shape, val_df_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea14014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df = pd.read_csv(f'../../input/pseudo/{folder}_data_{NTB}.csv') \n",
    "\n",
    "val_df = pd.read_csv('../../input/data_utils/train_5_folds.csv')\n",
    "val_df['label_many'] = val_df.image.apply(lambda x:  f\"labels_many/train_data/{x.split('/')[-2]}/label_many.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48144f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0acc817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFGS1 = [\n",
    "    {\n",
    "        'model_name': 'Unet',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'img_size': [512, 512],\n",
    "        'num_classes': 1,\n",
    "        'model_pth':  '/home/rohits/pv1/Contrail_Detection/output1/exp_10_s3/Unet/efficientnet-b7-512/checkpoint_dice_ctrl_fold0.pth',\n",
    "        'threshold': 0.24, #0.24,\n",
    "        'call_sign': \"roh_07_tta\",\n",
    "        'model_func': load_modelr1,\n",
    "        'tta': True,\n",
    "        'normalize': False\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a10c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc74b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "646ce6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'efficientnet-b7', 'img_size': [512, 512], 'num_classes': 1, 'model_pth': '/home/rohits/pv1/Contrail_Detection/output1/exp_10_s3/Unet/efficientnet-b7-512/checkpoint_dice_ctrl_fold0.pth', 'threshold': 0.24, 'call_sign': 'roh_07_tta', 'model_func': <function load_modelr1 at 0x7fb7c2520dc0>, 'tta': True, 'normalize': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b60b65027548aa9fd7b9bdaa426df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ../../output/pseudo_preds4/train does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 59\u001b[0m\n\u001b[1;32m     54\u001b[0m         preds\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39msqueeze(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     58\u001b[0m model_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()  \n\u001b[0;32m---> 59\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../output/pseudo_preds\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mNTB\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfolder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcfg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcall_sign\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m     63\u001b[0m model_masks \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(masks_, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     64\u001b[0m model_preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/serialization.py:315\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    314\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/serialization.py:288\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ../../output/pseudo_preds4/train does not exist."
     ]
    }
   ],
   "source": [
    "final_preds = []\n",
    "\n",
    "for idx, cfg in enumerate(CFGS1):   \n",
    "\n",
    "#     if idx <= 7:\n",
    "#         continue\n",
    "        \n",
    "    print(cfg)\n",
    "    val_transform = get_transform(cfg['img_size'])\n",
    "    val_transform2 = get_transform2(cfg['img_size'])\n",
    "\n",
    "    valid_dataset = ContrailDataset(val_df, transform=val_transform, normalize=False)  \n",
    "    if cfg['normalize'] and (\"ioa_\" in cfg['call_sign']):\n",
    "        valid_dataset = ContrailDataset(val_df, transform=val_transform2, normalize=True)  \n",
    "\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = 32, #32, \n",
    "        shuffle = False, \n",
    "        num_workers = 4, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model_base = cfg['model_func'](cfg)        \n",
    "      \n",
    "    if cfg['tta']:\n",
    "        if \"roh_\" in cfg['call_sign']:\n",
    "            model = tta.SegmentationTTAWrapper(model_base, tta.aliases.flip_transform(), merge_mode='mean')\n",
    "        else:\n",
    "            model = tta.SegmentationTTAWrapper(model_base, tta.aliases.hflip_transform(), merge_mode='mean')\n",
    "    \n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "    preds = []\n",
    "    masks_ = []\n",
    "        \n",
    "    for index, (images, masks) in enumerate(tqdm(valid_loader)):  \n",
    "\n",
    "        images  = images.to(device, dtype=torch.float)\n",
    "        masks  = masks.to(device, dtype=torch.float)\n",
    "        if cfg['img_size'][0] != 256:\n",
    "            masks = torch.nn.functional.interpolate(masks, size=256, mode='nearest') \n",
    "        masks_.append(torch.squeeze(masks, dim=1))\n",
    "        with torch.inference_mode():\n",
    "            images = torch.nn.functional.interpolate(images, size=cfg['img_size'][0], mode='nearest')\n",
    "            pred = model(images).sigmoid()   \n",
    "            pred = torch.nn.functional.interpolate(pred, size=256, mode='nearest')\n",
    "            preds.append(torch.squeeze(pred, dim=1))\n",
    "        \n",
    "            \n",
    "            \n",
    "    model_preds = torch.cat(preds, dim=0).detach().cpu()  \n",
    "#     torch.save(model_preds, f\"../../output/pseudo_preds{NTB}/{folder}/{cfg['call_sign']}.pt\")    \n",
    "\n",
    "    \n",
    "    \n",
    "    model_masks = torch.cat(masks_, dim=0)\n",
    "    model_preds = torch.cat(preds, dim=0)\n",
    "        \n",
    "#     model_masks = torch.flatten(model_masks, start_dim=0, end_dim=1)\n",
    "#     model_preds = torch.flatten(model_preds, start_dim=0, end_dim=1)  \n",
    "    \n",
    "#     # save\n",
    "#     torch.save(model_preds, f\"../../output/final_preds/{cfg['call_sign']}.pt\")    \n",
    "    \n",
    "#     best_threshold = 0.0\n",
    "#     best_dice_score = 0.0\n",
    "#     for threshold in [i / 100 for i in range(101)] :\n",
    "#         score = dice_coef(model_masks, model_preds, thr=threshold).cpu().detach().numpy() \n",
    "\n",
    "        \n",
    "#         if score > best_dice_score:\n",
    "#             best_dice_score = score\n",
    "#             best_threshold = threshold\n",
    "    \n",
    "        \n",
    "#     print(best_dice_score, best_threshold)\n",
    "#     final_preds1.append(model_preds)\n",
    "    \n",
    "    \n",
    "    final_preds.append(model_preds)\n",
    "    \n",
    "    if cfg['tta']: del model\n",
    "    del model_base\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "406b0219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Dice 0.71164286 0.04\n",
      "Global Dice 0.7223227 0.5\n"
     ]
    }
   ],
   "source": [
    "# best_threshold = 0.0\n",
    "# best_dice_score = 0.0\n",
    "# for threshold in [i / 100 for i in range(101)] :\n",
    "#     score = dice_coef(model_masks.detach().cpu(), model_preds.detach().cpu(), thr=threshold).cpu().detach().numpy() \n",
    "#     if score > best_dice_score:\n",
    "#         best_dice_score = score\n",
    "#         best_threshold = threshold\n",
    "\n",
    "print(\"Global Dice\", best_dice_score, best_threshold)\n",
    "score = dice_coef(model_masks.detach().cpu(), model_preds.detach().cpu(), thr=0.5).cpu().detach().numpy() \n",
    "print(\"Global Dice\", score, 0.5)\n",
    "\n",
    "scores = []\n",
    "for pred, mask in  zip(model_preds, model_masks):\n",
    "    scores.append(dice_coef(mask, pred, thr=0.5).cpu().detach().numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0e69971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['scores'] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "afe31f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.to_csv(\"../../input/data_utils/train_5_folds.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f8d76c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = val_df.loc[val_df.scores < 0.5].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88c2a4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1446\n",
       "0     694\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.scores.value_counts()\n",
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "654efa98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>class</th>\n",
       "      <th>label_many</th>\n",
       "      <th>id</th>\n",
       "      <th>fold</th>\n",
       "      <th>scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_data/5747024955383921098/image.npy</td>\n",
       "      <td>train_data/5747024955383921098/label.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>labels_many/train_data/5747024955383921098/lab...</td>\n",
       "      <td>5747024955383921098</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.086956e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_data/432548070956515051/image.npy</td>\n",
       "      <td>train_data/432548070956515051/label.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>labels_many/train_data/432548070956515051/labe...</td>\n",
       "      <td>432548070956515051</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.151515e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_data/3300848841428176436/image.npy</td>\n",
       "      <td>train_data/3300848841428176436/label.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>labels_many/train_data/3300848841428176436/lab...</td>\n",
       "      <td>3300848841428176436</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.999998e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_data/3374031604530790277/image.npy</td>\n",
       "      <td>train_data/3374031604530790277/label.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>labels_many/train_data/3374031604530790277/lab...</td>\n",
       "      <td>3374031604530790277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.959184e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_data/9131223346423158425/image.npy</td>\n",
       "      <td>train_data/9131223346423158425/label.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>labels_many/train_data/9131223346423158425/lab...</td>\n",
       "      <td>9131223346423158425</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.246377e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2915</th>\n",
       "      <td>train_data/489440619736314180/image.npy</td>\n",
       "      <td>train_data/489440619736314180/label.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>labels_many/train_data/489440619736314180/labe...</td>\n",
       "      <td>489440619736314180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.875706e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2916</th>\n",
       "      <td>train_data/1392568856138232946/image.npy</td>\n",
       "      <td>train_data/1392568856138232946/label.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>labels_many/train_data/1392568856138232946/lab...</td>\n",
       "      <td>1392568856138232946</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.723370e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2917</th>\n",
       "      <td>train_data/4281530218709087779/image.npy</td>\n",
       "      <td>train_data/4281530218709087779/label.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>labels_many/train_data/4281530218709087779/lab...</td>\n",
       "      <td>4281530218709087779</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.195531e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>train_data/5444185206942894271/image.npy</td>\n",
       "      <td>train_data/5444185206942894271/label.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>labels_many/train_data/5444185206942894271/lab...</td>\n",
       "      <td>5444185206942894271</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.829788e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2919</th>\n",
       "      <td>train_data/8443915190215904823/image.npy</td>\n",
       "      <td>train_data/8443915190215904823/label.npy</td>\n",
       "      <td>1</td>\n",
       "      <td>labels_many/train_data/8443915190215904823/lab...</td>\n",
       "      <td>8443915190215904823</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.125000e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2920 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         image  \\\n",
       "0     train_data/5747024955383921098/image.npy   \n",
       "1      train_data/432548070956515051/image.npy   \n",
       "2     train_data/3300848841428176436/image.npy   \n",
       "3     train_data/3374031604530790277/image.npy   \n",
       "4     train_data/9131223346423158425/image.npy   \n",
       "...                                        ...   \n",
       "2915   train_data/489440619736314180/image.npy   \n",
       "2916  train_data/1392568856138232946/image.npy   \n",
       "2917  train_data/4281530218709087779/image.npy   \n",
       "2918  train_data/5444185206942894271/image.npy   \n",
       "2919  train_data/8443915190215904823/image.npy   \n",
       "\n",
       "                                         label  class  \\\n",
       "0     train_data/5747024955383921098/label.npy      1   \n",
       "1      train_data/432548070956515051/label.npy      1   \n",
       "2     train_data/3300848841428176436/label.npy      1   \n",
       "3     train_data/3374031604530790277/label.npy      1   \n",
       "4     train_data/9131223346423158425/label.npy      1   \n",
       "...                                        ...    ...   \n",
       "2915   train_data/489440619736314180/label.npy      1   \n",
       "2916  train_data/1392568856138232946/label.npy      1   \n",
       "2917  train_data/4281530218709087779/label.npy      1   \n",
       "2918  train_data/5444185206942894271/label.npy      1   \n",
       "2919  train_data/8443915190215904823/label.npy      1   \n",
       "\n",
       "                                             label_many                   id  \\\n",
       "0     labels_many/train_data/5747024955383921098/lab...  5747024955383921098   \n",
       "1     labels_many/train_data/432548070956515051/labe...   432548070956515051   \n",
       "2     labels_many/train_data/3300848841428176436/lab...  3300848841428176436   \n",
       "3     labels_many/train_data/3374031604530790277/lab...  3374031604530790277   \n",
       "4     labels_many/train_data/9131223346423158425/lab...  9131223346423158425   \n",
       "...                                                 ...                  ...   \n",
       "2915  labels_many/train_data/489440619736314180/labe...   489440619736314180   \n",
       "2916  labels_many/train_data/1392568856138232946/lab...  1392568856138232946   \n",
       "2917  labels_many/train_data/4281530218709087779/lab...  4281530218709087779   \n",
       "2918  labels_many/train_data/5444185206942894271/lab...  5444185206942894271   \n",
       "2919  labels_many/train_data/8443915190215904823/lab...  8443915190215904823   \n",
       "\n",
       "      fold        scores  \n",
       "0      1.0  1.086956e-08  \n",
       "1      0.0  5.151515e-01  \n",
       "2      4.0  4.999998e-07  \n",
       "3      0.0  5.959184e-01  \n",
       "4      1.0  7.246377e-09  \n",
       "...    ...           ...  \n",
       "2915   0.0  5.875706e-01  \n",
       "2916   4.0  5.723370e-01  \n",
       "2917   3.0  5.195531e-01  \n",
       "2918   2.0  3.829788e-01  \n",
       "2919   3.0  3.125000e-08  \n",
       "\n",
       "[2920 rows x 7 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfe956de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1909e95c24134d6bb2456c8900bd7c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# NTB = 3\n",
    "call_signs = [\n",
    "    \"roh_01_tta\", \"nir_01_tta\", \"nir_02_tta\", \"nir_03_tta\", \"nir_04_tta\", \"ioa_01\", \"ioa_02\"\n",
    "]\n",
    "\n",
    "\n",
    "final_preds = []\n",
    "for sign in tqdm(call_signs, total=len(call_signs)):\n",
    "    wt = torch.load(f\"/home/rohits/pv1/Contrail_Detection/output/pseudo_preds{NTB}/{folder}/{sign}.pt\") \n",
    "    final_preds.append(wt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4605a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d231b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = torch.stack(final_preds).mean(dim=0)\n",
    "final_preds = (final_preds>0.35).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1667e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['id'] = val_df['image'].apply(lambda x: x.split(\"/\")[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7eff5a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e7666e5d8046b8a46f446c1074d26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1856 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = val_df['id'].values\n",
    "for (val, label_id) in tqdm(zip(final_preds, ids), total=len(ids)): \n",
    "    mask = val.view(256, 256, 1).detach().cpu().numpy()\n",
    "    np.save(f\"../../input/pseudo/{folder}_data_{NTB}/{label_id}/label_6811_702lb.npy\", mask.astype('float16')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950cbca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb905112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dcfd3543cb4d638b4471fef353fae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6510141\n",
      "0.66153294\n",
      "0.649828\n",
      "0.6697093\n",
      "0.6561677\n",
      "0.66287154\n",
      "0.6537375\n",
      "0.5 TH Score:  0.6811164\n",
      "0.6867434 0.35\n"
     ]
    }
   ],
   "source": [
    "# 0.657336 0.01\n",
    "\n",
    "\n",
    "# call_signs1 = [\n",
    "#     \"roh_01_tta\", \"nir_01_tta\", \"nir_02_tta\", \"nir_03_tta\", \"nir_04_tta\"\n",
    "# ] \n",
    "\n",
    "# call_signs1 = [\n",
    "#     \"roh_02_tta\",  \"roh_03_tta\", \"roh_04_tta\", \"roh_05_tta\", \"roh_06_tta\",  \"roh_07_tta\",  \"roh_08_tta\"  \n",
    "# ] \n",
    "\n",
    "call_signs1 = [\n",
    "    \"roh_01_tta\", \n",
    "    \"nir_01_tta\",\n",
    "    \"nir_02_tta\",\n",
    "    \"nir_03_tta\",\n",
    "    \"nir_04_tta\",\n",
    "    \n",
    "    \"ioa_01\", \n",
    "    \"ioa_02\",\n",
    "    \n",
    "] \n",
    "\n",
    "model_masks = torch.load(f\"/home/rohits/pv1/Contrail_Detection/output/final_preds/val_masks.pt\")\n",
    "\n",
    "\n",
    "preds1 = []\n",
    "\n",
    "for idx, sign in tqdm(enumerate(call_signs1), total=len(call_signs1)):\n",
    "    wt = torch.load(f\"/home/rohits/pv1/Contrail_Detection/output/final_preds/{sign}.pt\")    \n",
    "    preds1.append(wt)\n",
    "    \n",
    "    score = dice_coef(model_masks, wt, thr=0.5).cpu().detach().numpy() \n",
    "    print(score)\n",
    "    \n",
    "    \n",
    "# preds1.append(preds2)\n",
    "    \n",
    "final_preds = preds1\n",
    "final_preds = torch.stack(final_preds).mean(dim=0)\n",
    "score = dice_coef(model_masks, final_preds, thr=0.5).cpu().detach().numpy() \n",
    "\n",
    "print(\"0.5 TH Score: \", score)\n",
    "\n",
    "\n",
    "best_threshold = 0.0\n",
    "best_dice_score = 0.0\n",
    "for threshold in [i / 100 for i in range(101)] :\n",
    "    score = dice_coef(model_masks, final_preds, thr=threshold).cpu().detach().numpy() \n",
    "    if score > best_dice_score:\n",
    "        best_dice_score = score\n",
    "        best_threshold = threshold\n",
    "print(best_dice_score, best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73749c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5 TH Score:  0.67887855\n",
    "# 0.6856482 0.28\n",
    "\n",
    "\n",
    "\n",
    "# 0.5 TH Score:  0.6811164\n",
    "# 0.6867434 0.35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a82ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
