{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041ac14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "# !pip install --upgrade segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77e5af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import gc\n",
    "import pandas as pd\n",
    "import ttach as tta\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import cv2\n",
    "from torch.cuda import amp\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torchvision.utils import make_grid\n",
    "import optuna\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157ff8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e6bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb77d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"train\"  # \"validation\"\n",
    "    \n",
    "NTB = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5654e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ddaafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d58a5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "class ContrailDataset:\n",
    "    def __init__(self, df, transform=None, normalize=False):\n",
    "        self.df = df  \n",
    "        self.images = df['image']\n",
    "        self.labels = df['label']\n",
    "        self.transform =transform\n",
    "        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        self.normalize=normalize\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(\"../../input/\" + self.images[idx]).astype(float)   \n",
    "#         label = np.load(\"../../input/\" + self.labels[idx]).astype(float)\n",
    "        \n",
    "\n",
    "        if self.transform :\n",
    "#             data = self.transform(image=image, mask=label)\n",
    "            data = self.transform(image=image)\n",
    "            image  = data['image']\n",
    "#             label  = data['mask']\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "#             label = np.transpose(label, (2, 0, 1))    \n",
    "            \n",
    "            \n",
    "#         return torch.tensor(image), torch.tensor(label)\n",
    "    \n",
    "        if self.normalize:\n",
    "            image = self.normalize_image(torch.tensor(image))\n",
    "            return image\n",
    "        else:\n",
    "            return torch.tensor(image)\n",
    "    \n",
    "    \n",
    "# class ContrailDataset:\n",
    "#     def __init__(self, df, transform=None):\n",
    "#         self.df = df  \n",
    "#         self.images = df['image']\n",
    "#         self.transform =transform\n",
    "        \n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         image = np.load(\"../../input/\" + self.images[idx]).astype(float)   \n",
    "# #         label = np.load(\"../../input\" + self.labels[idx]).astype(float)\n",
    "        \n",
    "        \n",
    "#         # label_cls = 1 if label.sum() > 0 else 0\n",
    "#         if self.transform :\n",
    "#             data = self.transform(image=image)\n",
    "#             image  = data['image']\n",
    "#             image = np.transpose(image, (2, 0, 1))\n",
    "            \n",
    "#         return torch.tensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "985b3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###TImmunetplusplus model Nirjhar\n",
    "\n",
    "n_blocks = 4\n",
    "\n",
    "class TimmSegModel(nn.Module):\n",
    "    def __init__(self, cfg, segtype='unet', pretrained=True):\n",
    "        super(TimmSegModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(12, 36, 3, stride=1, padding=1, bias=False)\n",
    "        self.mybn1 = nn.BatchNorm2d(6)\n",
    "        self.mybn2 = nn.BatchNorm2d(12)\n",
    "        self.mybn3 = nn.BatchNorm2d(36)     \n",
    "        self.encoder = timm.create_model(\n",
    "            cfg['backbone'],\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=0.8,\n",
    "            drop_path_rate=0.5,\n",
    "            pretrained=False\n",
    "        )\n",
    "        self.encoder.conv_stem=nn.Conv2d(6, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        self.encoder.blocks[5] = nn.Identity()\n",
    "        self.encoder.blocks[6] = nn.Sequential(\n",
    "            nn.Conv2d(self.encoder.blocks[4][2].conv_pwl.out_channels, 320, 1),\n",
    "            nn.BatchNorm2d(320),\n",
    "            nn.ReLU6(),\n",
    "        )\n",
    "        tr = torch.randn(1,6,64,64)\n",
    "        g = self.encoder(tr)\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.decoders.unetplusplus.decoder.UnetPlusPlusDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Conv2d(\n",
    "            decoder_channels[n_blocks-1],\n",
    "            cfg[\"num_classes\"], \n",
    "            kernel_size=(3, 3),\n",
    "            stride=(1, 1), \n",
    "            padding=(1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu6(self.mybn1(self.conv1(x)))\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "    \n",
    "\n",
    "def load_model1(cfg):\n",
    "    model = TimmSegModel(cfg)\n",
    "    model.load_state_dict(torch.load(cfg['model_pth']))\n",
    "    return model\n",
    "\n",
    "\n",
    "#### Model 2 Nirjhar\n",
    "\n",
    "n_blocks =4\n",
    "class TimmSegModel2(nn.Module):\n",
    "    def __init__(self, cfg, segtype='unet', pretrained=True):\n",
    "        super(TimmSegModel2, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            cfg['backbone'],\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=0.5,\n",
    "            pretrained=False\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 128, 128))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.decoders.unetplusplus.decoder.UnetPlusPlusDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv2d(decoder_channels[n_blocks-1], 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "    \n",
    "\n",
    "def load_model2(cfg):\n",
    "    model = TimmSegModel2(cfg)\n",
    "    model.load_state_dict(torch.load(cfg['model_pth']))\n",
    "    return model\n",
    "\n",
    "\n",
    "##################\n",
    "\n",
    "n_blocks = 4\n",
    "\n",
    "class TimmSegModel3(nn.Module):\n",
    "    def __init__(self, cfg, segtype='unet', pretrained=True):\n",
    "        super(TimmSegModel3, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(6, 12, 3, stride=1, padding=1, bias=False)\n",
    "        self.conv3 = nn.Conv2d(12, 36, 3, stride=1, padding=1, bias=False)\n",
    "        self.mybn1 = nn.BatchNorm2d(6)\n",
    "        self.mybn2 = nn.BatchNorm2d(12)\n",
    "        self.mybn3 = nn.BatchNorm2d(36)     \n",
    "        self.encoder = timm.create_model(\n",
    "            cfg[\"backbone\"],\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=0.8,\n",
    "            drop_path_rate=0.5,\n",
    "            pretrained=False\n",
    "        )\n",
    "        self.encoder.conv_stem=nn.Conv2d(6, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "\n",
    "        self.encoder.blocks[5] = nn.Identity()\n",
    "        self.encoder.blocks[6] = nn.Sequential(\n",
    "            nn.Conv2d(self.encoder.blocks[4][2].conv_pwl.out_channels, 320, 1),\n",
    "            nn.BatchNorm2d(320),\n",
    "            nn.ReLU6(),\n",
    "        )\n",
    "        tr = torch.randn(1,6,64,64)\n",
    "        g = self.encoder(tr)\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.decoders.unetplusplus.decoder.UnetPlusPlusDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Conv2d(decoder_channels[n_blocks-1], cfg['num_classes'], kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu6(self.mybn1(self.conv1(x)))\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "    \n",
    "    \n",
    "    \n",
    "def load_model3(cfg):\n",
    "    model = TimmSegModel3(cfg)\n",
    "    model.load_state_dict(torch.load(cfg['model_pth']))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "class TimmSegModel4(nn.Module):\n",
    "    def __init__(self, cfg, segtype='unet', pretrained=True):\n",
    "        super(TimmSegModel4, self).__init__()\n",
    "\n",
    "        self.encoder = timm.create_model(\n",
    "            cfg[\"backbone\"],\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=0.5,\n",
    "            pretrained=False\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 512, 512))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.decoders.unetplusplus.decoder.UnetPlusPlusDecoder(\n",
    "                encoder_channels=encoder_channels[:n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:n_blocks],\n",
    "                n_blocks=n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv2d(decoder_channels[n_blocks-1], 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = [0] + self.encoder(x)[:n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "    \n",
    "    \n",
    "def load_model4(cfg):\n",
    "    model = TimmSegModel4(cfg)\n",
    "    model.load_state_dict(torch.load(cfg['model_pth']))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Rohit Model \n",
    "class Net_R(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=cfg[\"backbone\"],     \n",
    "            encoder_weights=None,   \n",
    "            in_channels=3,  \n",
    "            classes=cfg[\"num_classes\"],\n",
    "            activation=None\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        mask = self.model(inputs)\n",
    "        return mask\n",
    "    \n",
    "    \n",
    "def load_modelr1(cfg):\n",
    "    model = Net_R(cfg)\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    model.load_state_dict(torch.load(cfg['model_pth'], map_location=torch.device('cpu'))['model'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#     if idx in [11, 12]:\n",
    "#         model = TimmSegModelR1(cfg)\n",
    "#         model = torch.nn.DataParallel(model).cuda()\n",
    "#         model.load_state_dict(torch.load(cfg['model_pth'], map_location=torch.device('cpu'))['model'])\n",
    "#     else:\n",
    "#         model = Net(cfg)    \n",
    "#         model = torch.nn.DataParallel(model).cuda()\n",
    "#         model.load_state_dict(torch.load(cfg['model_pth'], map_location=torch.device('cpu'))['model'])\n",
    "\n",
    "class TimmSegModelR1(nn.Module):\n",
    "    def __init__(self, cfg, segtype='unet', pretrained=True):\n",
    "        super(TimmSegModelR1, self).__init__()\n",
    "\n",
    "        self.n_blocks = 4\n",
    "        self.encoder = timm.create_model(\n",
    "            cfg['backbone'],\n",
    "            in_chans=3,\n",
    "            features_only=True,\n",
    "            drop_rate=0.5,\n",
    "            pretrained=False\n",
    "        )\n",
    "        g = self.encoder(torch.rand(1, 3, 128, 128))\n",
    "        encoder_channels = [1] + [_.shape[1] for _ in g]\n",
    "        decoder_channels = [256, 128, 64, 32, 16]\n",
    "        if segtype == 'unet':\n",
    "            self.decoder = smp.decoders.unetplusplus.decoder.UnetPlusPlusDecoder(\n",
    "                encoder_channels=encoder_channels[:self.n_blocks+1],\n",
    "                decoder_channels=decoder_channels[:self.n_blocks],\n",
    "                n_blocks=self.n_blocks,\n",
    "            )\n",
    "\n",
    "        self.segmentation_head = nn.Sequential(\n",
    "            nn.Conv2d(decoder_channels[self.n_blocks-1], \n",
    "                      cfg['num_classes'],\n",
    "                      kernel_size=(3, 3),\n",
    "                      stride=(1, 1),\n",
    "                      padding=(1, 1)\n",
    "            ),\n",
    "            nn.UpsamplingBilinear2d(scale_factor=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = [0] + self.encoder(x)[:self.n_blocks]\n",
    "        seg_features = self.decoder(*global_features)\n",
    "        seg_features = self.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "\n",
    "\n",
    "    \n",
    "def load_modelr2(cfg):\n",
    "    model = TimmSegModelR1(cfg)\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    model.load_state_dict(torch.load(cfg['model_pth'], map_location=torch.device('cpu'))['model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5aebd",
   "metadata": {},
   "source": [
    "### Ioannis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f0fd672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 2.0.1+cu117\n",
      "Segmentation Models version: 0.3.3\n",
      "Timm version: 0.9.2\n"
     ]
    }
   ],
   "source": [
    "### -----------------------------------------\n",
    "### exp28 Senet154 + deep_supervision \n",
    "### -----------------------------------------\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "# from einops import rearrange, reduce, repeat\n",
    "from segmentation_models_pytorch.decoders.unet.decoder import UnetDecoder, DecoderBlock\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import pickle \n",
    "import glob\n",
    "import pytorch_lightning as pl\n",
    "import timm \n",
    "\n",
    "print(f\"Pytorch version: {torch.__version__}\")\n",
    "print(f\"Segmentation Models version: {smp.__version__}\")\n",
    "print(f\"Timm version: {timm.__version__}\")\n",
    "\n",
    "\n",
    "#################################\n",
    "##### Dataset \n",
    "################################\n",
    "\n",
    "\n",
    "class DatasetExp28(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_size=256, train=True, normalize=True):\n",
    "        \n",
    "        self.df = df\n",
    "        self.trn = train\n",
    "        self.df_idx: pd.DataFrame = pd.DataFrame({'idx': os.listdir(f'/kaggle/input/google-research-identify-contrails-reduce-global-warming/test')})\n",
    "        self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        self.image_size = image_size\n",
    "        self.normalize = normalize\n",
    "        if image_size != 256:\n",
    "            self.resize_image = T.transforms.Resize(image_size)\n",
    "        \n",
    "        #self.images = df['image']\n",
    "    \n",
    "    def read_record(self, directory):\n",
    "        record_data = {}\n",
    "        for x in [\n",
    "            \"band_11\", \n",
    "            \"band_14\", \n",
    "            \"band_15\"\n",
    "        ]:\n",
    "\n",
    "            record_data[x] = np.load(os.path.join(directory, x + \".npy\"))\n",
    "\n",
    "        return record_data\n",
    "\n",
    "    def normalize_range(self, data, bounds):\n",
    "        \"\"\"Maps data to the range [0, 1].\"\"\"\n",
    "        return (data - bounds[0]) / (bounds[1] - bounds[0])\n",
    "    \n",
    "    def get_false_color(self, record_data):\n",
    "        _T11_BOUNDS = (243, 303)\n",
    "        _CLOUD_TOP_TDIFF_BOUNDS = (-4, 5)\n",
    "        _TDIFF_BOUNDS = (-4, 2)\n",
    "        \n",
    "        N_TIMES_BEFORE = 4\n",
    "\n",
    "        r = self.normalize_range(record_data[\"band_15\"] - record_data[\"band_14\"], _TDIFF_BOUNDS)\n",
    "        g = self.normalize_range(record_data[\"band_14\"] - record_data[\"band_11\"], _CLOUD_TOP_TDIFF_BOUNDS)\n",
    "        b = self.normalize_range(record_data[\"band_14\"], _T11_BOUNDS)\n",
    "        false_color = np.clip(np.stack([r, g, b], axis=2), 0, 1)\n",
    "        img = false_color[..., N_TIMES_BEFORE]\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        con_path = row.path\n",
    "        data = self.read_record(con_path)\n",
    "        img = self.get_false_color(data)\n",
    "        \n",
    "        ###### Rohit dataset -- todo \n",
    "        #img = np.load(self.images[index]).astype(float)\n",
    "        \n",
    "        img = torch.tensor(np.reshape(img, (256, 256, 3))).to(torch.float32).permute(2, 0, 1)\n",
    "        \n",
    "        if self.image_size != 256:\n",
    "            img = self.resize_image(img)\n",
    "            \n",
    "        if self.normalize:\n",
    "            img = self.normalize_image(img)\n",
    "        \n",
    "        #image_id = int(self.df_idx.iloc[index]['idx'])\n",
    "        return img.float() #, torch.tensor(image_id)\n",
    "    \n",
    "\n",
    "    \n",
    "#############################    \n",
    "### MODEL    \n",
    "#############################\n",
    "class SmpUnetDecoder(nn.Module):\n",
    "    def __init__(self,in_channel,skip_channel,out_channel):\n",
    "        super().__init__()\n",
    "        self.center = nn.Identity()\n",
    "        i_channel = [in_channel,]+ out_channel[:-1]\n",
    "        s_channel = skip_channel\n",
    "        o_channel = out_channel\n",
    "        block = [\n",
    "            DecoderBlock(i, s, o, use_batchnorm=True, attention_type=None)\n",
    "            for i, s, o in zip(i_channel, s_channel, o_channel)\n",
    "        ]\n",
    "        self.block = nn.ModuleList(block)\n",
    "        \n",
    "    def forward(self, feature, skip):\n",
    "        d = self.center(feature)\n",
    "        decode = []\n",
    "        for i, block in enumerate(self.block):\n",
    "            s = skip[i]\n",
    "            d = block(d, s)\n",
    "            decode.append(d)\n",
    "        last  = d\n",
    "        return last, decode\n",
    "\n",
    "\n",
    "def conv3x3(in_channel, out_channel): #not change resolusion\n",
    "    return nn.Conv2d(in_channel,out_channel,\n",
    "                      kernel_size=3,stride=1,padding=1,dilation=1,bias=False)\n",
    "\n",
    "def conv1x1(in_channel, out_channel): #not change resolution\n",
    "    return nn.Conv2d(in_channel,out_channel,\n",
    "                      kernel_size=1,stride=1,padding=0,dilation=1,bias=False)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self, cfg, vb=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.vb = vb\n",
    "        self.crop_depth = None #cfg.crop_depth\n",
    "        self.deepsupervision = False #cfg.deepsupervision #True\n",
    "        \n",
    "        # if cfg.backbone == 'resnet34' or cfg.backbone == 'seresnet34':\n",
    "        if 'resnet34' in cfg.backbone:\n",
    "            conv_dim=64\n",
    "            encoder_dim = [conv_dim, 64, 128, 256, 512, ]\n",
    "            decoder_dim = [256, 128, 64, 32, 16]\n",
    "        else:\n",
    "            conv_dim=128 #128\n",
    "            encoder_dim  = [conv_dim] + [256, 512, 1024, 2048]\n",
    "            decoder_dim = [512, 256, 128, 64, 16]\n",
    "            \n",
    "        # self.encoder = resnet34d(pretrained=False,in_chans=CFG.one_depth)\n",
    "        self.encoder = timm.create_model(cfg.backbone, pretrained=cfg.pretrained, in_chans=cfg.in_chans, num_classes=0) #drop_path_rate=0.2, \n",
    "        self.decoder = SmpUnetDecoder(\n",
    "            in_channel=encoder_dim[-1],\n",
    "            skip_channel=encoder_dim[:-1][::-1] + [0],\n",
    "            out_channel=decoder_dim\n",
    "        )\n",
    "        ### seg head\n",
    "        seg_head_in_c = 16 #if 'resnet34' in cfg.backbone else 128\n",
    "        self.logit = nn.Conv2d(decoder_dim[-1], 1, kernel_size=1)\n",
    "        \n",
    "        #-- pool attention weight\n",
    "        self.weight = nn.ModuleList([\n",
    "            nn.Conv2d(encoder_dim[i], 1, kernel_size=1, padding=0) for i in range(len(encoder_dim))\n",
    "        ])\n",
    "        #### deep supervision\n",
    "        deep_ch = [16, 64, 128, 256]   # [64, 64, 64, 64]\n",
    "        self.deep4 = conv1x1(deep_ch[0],1)#.apply(init_weight)\n",
    "        self.deep3 = conv1x1(deep_ch[1],1)#.apply(init_weight)\n",
    "        self.deep2 = conv1x1(deep_ch[2],1)#.apply(init_weight)\n",
    "        self.deep1 = conv1x1(deep_ch[3],1)#.apply(init_weight)\n",
    "        \n",
    "        self.up1 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        self.up2 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        self.up3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up4 = nn.Upsample(scale_factor=1, mode='bilinear', align_corners=True)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        K = 1\n",
    "        x = batch\n",
    "        # ----\n",
    "        encoder = []\n",
    "        e = self.encoder\n",
    "        \n",
    "        x = e.conv1(x)\n",
    "        x = e.bn1(x)\n",
    "        x = e.act1(x)\n",
    "        encoder.append(x)\n",
    "        \n",
    "        x = F.avg_pool2d(x,kernel_size=2,stride=2)\n",
    "        x = e.layer1(x)\n",
    "        encoder.append(x)\n",
    "        \n",
    "        x = e.layer2(x)\n",
    "        encoder.append(x)\n",
    "        \n",
    "        x = e.layer3(x)\n",
    "        encoder.append(x)\n",
    "        \n",
    "        x = e.layer4(x)\n",
    "        encoder.append(x)\n",
    "        #if self.vb: print('encoder', [f.shape for f in encoder])\n",
    "\n",
    "        ### decoder\n",
    "        last, decoder = self.decoder(feature=encoder[-1], skip = encoder[:-1][::-1] + [None])\n",
    "        #if self.vb: print('decoder',[f.shape for f in decoder])\n",
    "        #if self.vb: print('last',last.shape)\n",
    "        \n",
    "        ### head\n",
    "        logit = self.logit(last)\n",
    "        #if self.vb: print('logit',logit.shape)\n",
    "        \n",
    "        if self.deepsupervision:\n",
    "            y4 = decoder[-1]  ### torch.Size([B, 16, 256, 256])\n",
    "            y3 = decoder[-2]  ### torch.Size([B, 32, 128, 128])\n",
    "            y2 = decoder[-3]  ### torch.Size([B, 64, 64, 64])\n",
    "            y1 = decoder[-4]  ### torch.Size([B, 128, 32, 32])\n",
    "            ### --> B x C x H x W\n",
    "            y1 = self.up1(y1)  ### x 8\n",
    "            y2 = self.up2(y2)  ### x 4 \n",
    "            y3 = self.up3(y3)  ### x 2\n",
    "            y4 = self.up4(y4)  ### x 1  \n",
    "            ##################\n",
    "            s4 = self.deep4(y4) ### --> B x 1 x H x W\n",
    "            s3 = self.deep3(y3)\n",
    "            s2 = self.deep2(y2)\n",
    "            s1 = self.deep1(y1)\n",
    "            logits_deeps = [s4,s3,s2,s1]\n",
    "            #if self.vb: print('logits_deeps', s1.shape,  s2.shape,  s3.shape,  s4.shape)\n",
    "            return logit, logits_deeps\n",
    "        return logit\n",
    "\n",
    "    \n",
    "class args_28:\n",
    "    size=512 \n",
    "    exp_name = \"exp28\"\n",
    "    use_folds = [0]\n",
    "    ##use_folds = [0,1,2,3]\n",
    "    in_chans = 3 #12 # 6 # 65\n",
    "    size = 512 #384 # 224\n",
    "    image_size = size\n",
    "    target_size = 1\n",
    "    batch_size = 8 #32\n",
    "    backbone = 'gluon_senet154'\n",
    "    seg_model = \"Unet\"   ## \"Unet++\", \"MAnet\", \"Linknet\", \"FPN\", \"PSPNet\", \"PAN\", \"DeepLabV3\",  \"DeepLabV3+\"\n",
    "    model_name = f'{seg_model}-{backbone}'\n",
    "    pretrained = False\n",
    "    deepsupervision=False \n",
    "\n",
    "    \n",
    "    \n",
    "class LightningModule_28(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Net(args_28) \n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "\n",
    "    \n",
    "def load_model_exp28(cfg):\n",
    "    model = LightningModule_28().load_from_checkpoint(cfg['model_pth']) \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "### ensemble muiltiple checkpoints \n",
    "\n",
    "\n",
    "### move all weights to nb \n",
    "MODEL_DIR_EXP28 = \"/kaggle/input/contrails-exp28b-senet154-512/\"    \n",
    "CKPTS_EXP28 = [\n",
    "    #     ######## 4-skf split \n",
    "    #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0.ckpt',\n",
    "    #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v1.ckpt',\n",
    "    #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v2.ckpt',\n",
    "    #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v3.ckpt',\n",
    "    #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v4.ckpt',\n",
    "    #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v5.ckpt'\n",
    "    #     ####### full train.csv \n",
    "    '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold-1.ckpt',\n",
    "    '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold-1-v1.ckpt',\n",
    "    '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold-1-v2.ckpt',\n",
    "]\n",
    "\n",
    "\n",
    "class EnsembleModel_28(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList()\n",
    "        \n",
    "        for i, model_path in enumerate(cfg['model_pth']):\n",
    "            _model = LightningModule_28().load_from_checkpoint(model_path) #cfg['model_pth']\n",
    "            _model.eval()\n",
    "            print('Load weights from:', model_path)\n",
    "            self.model.append(_model)        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs=[]\n",
    "        for m in self.model:\n",
    "            outputs.append(m(x)) #(torch.sigmoid(out))\n",
    "        res = torch.stack(outputs,dim=0).mean(0)\n",
    "        return res  \n",
    "\n",
    "def load_model_exp28_snapshot(cfg):\n",
    "    model = EnsembleModel_28(cfg)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f06f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -----------------------------------------\n",
    "### exp27\n",
    "### -----------------------------------------\n",
    "\n",
    "\n",
    "class args_27:\n",
    "    in_chans = 3 \n",
    "    size = 512 #384 # 224\n",
    "    backbone = 'resnest26d'\n",
    "    seg_model = \"Unet\"   ## \"Unet++\", \"MAnet\", \"Linknet\", \"FPN\", \"PSPNet\", \"PAN\", \"DeepLabV3\",  \"DeepLabV3+\"\n",
    "    pretrained = False\n",
    "    deepsupervision=False \n",
    "\n",
    "\n",
    "\n",
    "class SmpUnetDecoder(nn.Module):\n",
    "    def __init__(self,in_channel,skip_channel,out_channel):\n",
    "        super().__init__()\n",
    "        self.center = nn.Identity()\n",
    "        i_channel = [in_channel,]+ out_channel[:-1]\n",
    "        s_channel = skip_channel\n",
    "        o_channel = out_channel\n",
    "        block = [\n",
    "            DecoderBlock(i, s, o, use_batchnorm=True, attention_type=None)\n",
    "            for i, s, o in zip(i_channel, s_channel, o_channel)\n",
    "        ]\n",
    "        self.block = nn.ModuleList(block)\n",
    "        \n",
    "    def forward(self, feature, skip):\n",
    "        d = self.center(feature)\n",
    "        decode = []\n",
    "        for i, block in enumerate(self.block):\n",
    "            s = skip[i]\n",
    "            d = block(d, s)\n",
    "            decode.append(d)\n",
    "        last  = d\n",
    "        return last, decode\n",
    "\n",
    "\n",
    "def conv3x3(in_channel, out_channel): #not change resolusion\n",
    "    return nn.Conv2d(in_channel,out_channel,\n",
    "                      kernel_size=3,stride=1,padding=1,dilation=1,bias=False)\n",
    "\n",
    "def conv1x1(in_channel, out_channel): #not change resolution\n",
    "    return nn.Conv2d(in_channel,out_channel,\n",
    "                      kernel_size=1,stride=1,padding=0,dilation=1,bias=False)\n",
    "    \n",
    "class Net_27(nn.Module):\n",
    "    def __init__(self, cfg, vb=False):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.vb = vb\n",
    "        self.crop_depth = None #cfg.crop_depth\n",
    "        self.deepsupervision = False #cfg.deepsupervision #True\n",
    "        \n",
    "        # if cfg.backbone == 'resnet34' or cfg.backbone == 'seresnet34':\n",
    "        if 'resnet34' in cfg.backbone:\n",
    "            conv_dim=64\n",
    "            encoder_dim = [conv_dim, 64, 128, 256, 512, ]\n",
    "            decoder_dim = [256, 128, 64, 32, 16]\n",
    "        else:\n",
    "            conv_dim=64 #128\n",
    "            encoder_dim  = [conv_dim] + [256, 512, 1024, 2048]\n",
    "            decoder_dim = [256, 128, 64, 32, 16]\n",
    "            \n",
    "        # self.encoder = resnet34d(pretrained=False,in_chans=CFG.one_depth)\n",
    "        self.encoder = timm.create_model(cfg.backbone, pretrained=False, in_chans=3, num_classes=0) #drop_path_rate=0.2, \n",
    "        self.decoder = SmpUnetDecoder(\n",
    "            in_channel=encoder_dim[-1],\n",
    "            skip_channel=encoder_dim[:-1][::-1] + [0],\n",
    "            out_channel=decoder_dim\n",
    "        )\n",
    "        ### seg head\n",
    "        seg_head_in_c = 16 #if 'resnet34' in cfg.backbone else 128\n",
    "        self.logit = nn.Conv2d(decoder_dim[-1], 1, kernel_size=1)\n",
    "        \n",
    "        #-- pool attention weight\n",
    "        self.weight = nn.ModuleList([\n",
    "            nn.Conv2d(encoder_dim[i], 1, kernel_size=1, padding=0) for i in range(len(encoder_dim))\n",
    "        ])\n",
    "        #### deep supervision\n",
    "        deep_ch = [16, 32, 64, 128] # [64, 64, 64, 64]\n",
    "        self.deep4 = conv1x1(deep_ch[0],1)#.apply(init_weight)\n",
    "        self.deep3 = conv1x1(deep_ch[1],1)#.apply(init_weight)\n",
    "        self.deep2 = conv1x1(deep_ch[2],1)#.apply(init_weight)\n",
    "        self.deep1 = conv1x1(deep_ch[3],1)#.apply(init_weight)\n",
    "        \n",
    "        self.up1 = nn.Upsample(scale_factor=8, mode='bilinear', align_corners=True)\n",
    "        self.up2 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        self.up3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.up4 = nn.Upsample(scale_factor=1, mode='bilinear', align_corners=True)\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        K = 1\n",
    "        x = batch\n",
    "        # ----\n",
    "        encoder = []\n",
    "        e = self.encoder\n",
    "        \n",
    "        x = e.conv1(x)\n",
    "        x = e.bn1(x)\n",
    "        x = e.act1(x)\n",
    "        encoder.append(x)\n",
    "        \n",
    "        x = F.avg_pool2d(x,kernel_size=2,stride=2)\n",
    "        x = e.layer1(x)\n",
    "        encoder.append(x)\n",
    "        \n",
    "        x = e.layer2(x)\n",
    "        encoder.append(x)\n",
    "        \n",
    "        x = e.layer3(x)\n",
    "        encoder.append(x)\n",
    "        \n",
    "        x = e.layer4(x)\n",
    "        encoder.append(x)\n",
    "        #if self.vb: print('encoder', [f.shape for f in encoder])\n",
    "\n",
    "        ### decoder\n",
    "        last, decoder = self.decoder(feature=encoder[-1], skip = encoder[:-1][::-1] + [None])\n",
    "        #if self.vb: print('decoder',[f.shape for f in decoder])\n",
    "        #if self.vb: print('last',last.shape)\n",
    "        \n",
    "        ### head\n",
    "        logit = self.logit(last)\n",
    "        #if self.vb: print('logit',logit.shape)\n",
    "        \n",
    "        if self.deepsupervision:\n",
    "            y4 = decoder[-1]  ### torch.Size([B, 16, 256, 256])\n",
    "            y3 = decoder[-2]  ### torch.Size([B, 32, 128, 128])\n",
    "            y2 = decoder[-3]  ### torch.Size([B, 64, 64, 64])\n",
    "            y1 = decoder[-4]  ### torch.Size([B, 128, 32, 32])\n",
    "            ### --> B x C x H x W\n",
    "            y1 = self.up1(y1)  ### x 8\n",
    "            y2 = self.up2(y2)  ### x 4 \n",
    "            y3 = self.up3(y3)  ### x 2\n",
    "            y4 = self.up4(y4)  ### x 1  \n",
    "            ##################\n",
    "            s4 = self.deep4(y4) ### --> B x 1 x H x W\n",
    "            s3 = self.deep3(y3)\n",
    "            s2 = self.deep2(y2)\n",
    "            s1 = self.deep1(y1)\n",
    "            logits_deeps = [s4,s3,s2,s1]\n",
    "            #if self.vb: print('logits_deeps', s1.shape,  s2.shape,  s3.shape,  s4.shape)\n",
    "            return logit, logits_deeps\n",
    "        return logit\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class LightningModule_27(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = Net_27(args_27) \n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "\n",
    "    \n",
    "class EnsembleModel_27(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList()\n",
    "        \n",
    "        #for fold, w in zip(use_folds, weights):\n",
    "        for i, model_path in enumerate(cfg['model_pth']):\n",
    "            \n",
    "            #model_path = CKPTS_EXP28[i]\n",
    "            _model = LightningModule_27().load_from_checkpoint(model_path) #cfg['model_pth']\n",
    "            _model.eval()\n",
    "            print('Load weights from:', model_path)\n",
    "\n",
    "            self.model.append(_model)\n",
    "            #self.weights.append(w)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs=[]\n",
    "        for m in self.model:\n",
    "            outputs.append(m(x)) #(torch.sigmoid(out))\n",
    "        res = torch.stack(outputs,dim=0).mean(0)\n",
    "        return res  \n",
    "\n",
    "def load_model_exp27_soup(cfg):\n",
    "    model = EnsembleModel_27(cfg)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faaf0aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -----------------------------------------\n",
    "### exp26 Unet++ regnetx_080\n",
    "### -----------------------------------------\n",
    "\n",
    "# class args:\n",
    "#     backbone = 'timm-regnetx_080'\n",
    "\n",
    "class SegModel_26(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegModel_26, self).__init__()\n",
    "        \n",
    "        self.seg = smp.UnetPlusPlus(encoder_name='timm-regnetx_080', encoder_weights=None, classes=1, activation=None)\n",
    "        ##self.seg = seg_models[cfg.seg_model](encoder_name=cfg.backbone, encoder_weights=\"imagenet\", classes=1, activation=None)\n",
    "\n",
    "    def forward(self,x):\n",
    "        global_features = self.seg.encoder(x)\n",
    "        seg_features = self.seg.decoder(*global_features)\n",
    "        seg_features = self.seg.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "    \n",
    "    \n",
    "class LightningModule_26(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SegModel_26()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "\n",
    "\n",
    "class EnsembleModel_26(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList()\n",
    "        \n",
    "        for i, model_path in enumerate(cfg['model_pth']):\n",
    "            _model = LightningModule_26().load_from_checkpoint(model_path) #cfg['model_pth']\n",
    "            _model.eval()\n",
    "            print('Load weights from:', model_path)\n",
    "            self.model.append(_model)\n",
    "            #self.weights.append(w)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs=[]\n",
    "        for m in self.model:\n",
    "            outputs.append(m(x)) #(torch.sigmoid(out))\n",
    "        res = torch.stack(outputs,dim=0).mean(0)\n",
    "        return res  \n",
    "\n",
    "    \n",
    "def load_model_exp26_soup(cfg):\n",
    "    model = EnsembleModel_26(cfg)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b790184",
   "metadata": {},
   "outputs": [],
   "source": [
    "### -----------------------------------------\n",
    "### exp19 Unet++\n",
    "### -----------------------------------------\n",
    "\n",
    "# class args:\n",
    "#     exp_name = \"exp19\"\n",
    "#     in_chans = 3 #12 # 6 # 65\n",
    "#     size = 256  \n",
    "#     target_size = 1\n",
    "#     batch_size = 32\n",
    "#     backbone = 'timm-resnest26d'\n",
    "\n",
    "\n",
    "class SegModel_19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegModel_19, self).__init__()\n",
    "        self.seg = smp.UnetPlusPlus(encoder_name='timm-resnest26d', encoder_weights=None, classes=1, activation=None)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        global_features = self.seg.encoder(x)\n",
    "        seg_features = self.seg.decoder(*global_features)\n",
    "        seg_features = self.seg.segmentation_head(seg_features)\n",
    "        return seg_features\n",
    "\n",
    "\n",
    "class LightningModule_19(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SegModel_19()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(batch)\n",
    "\n",
    "    \n",
    "class EnsembleModel_19(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.model = nn.ModuleList()\n",
    "        \n",
    "        for i, model_path in enumerate(cfg['model_pth']):\n",
    "            _model = LightningModule_19().load_from_checkpoint(model_path) #cfg['model_pth']\n",
    "            _model.eval()\n",
    "            print('Load weights from:', model_path)\n",
    "            self.model.append(_model)\n",
    "            #self.weights.append(w)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        outputs=[]\n",
    "        for m in self.model:\n",
    "            outputs.append(m(x)) #(torch.sigmoid(out))\n",
    "        res = torch.stack(outputs,dim=0).mean(0)\n",
    "        return res  \n",
    "\n",
    "    \n",
    "def load_model_exp19_soup(cfg):\n",
    "    model = EnsembleModel_19(cfg)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554e5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35da6d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07bfce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, thr=0.5, epsilon=1e-6):\n",
    "    y_true = y_true.to(torch.float32)\n",
    "    y_pred = (y_pred>thr).to(torch.float32)\n",
    "    inter = (y_true*y_pred).sum()\n",
    "    den = y_true.sum() + y_pred.sum()\n",
    "    dice = ((2*inter+epsilon)/(den+epsilon)).mean()\n",
    "    \n",
    "    return dice\n",
    "\n",
    "\n",
    "def get_transform(img_size):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(*img_size, interpolation=cv2.INTER_NEAREST),\n",
    "    ], p=1.0)\n",
    "    return transform\n",
    "\n",
    "def get_transform2(img_size):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(*img_size, interpolation=cv2.INTER_LINEAR),\n",
    "    ], p=1.0)\n",
    "    return transform\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01378f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df = pd.read_csv(\"../../input/data_utils/val_df_filled.csv\")\n",
    "# val_df_full = val_df.copy()\n",
    "# val_dups = np.load(\"../../input/data_utils/dups_val.npy\")\n",
    "# val_dups = [int(val_id) for val_id in val_dups]\n",
    "\n",
    "# val_df = val_df.loc[~val_df['id'].isin(val_dups)].reset_index(drop=True)\n",
    "# print(val_df.shape, val_df_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aea14014",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(f'../../input/pseudo/{folder}_data_{NTB}.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48144f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0acc817c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFGS1 = [\n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'efficientnet-b7',\n",
    "#         'img_size': [256, 256],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold0.pth',\n",
    "#         'threshold': 0.24, #0.24,\n",
    "#         'call_sign': \"roh_01_tta\",\n",
    "#         'model_func': load_modelr1,\n",
    "#         'tta': True,\n",
    "#         'normalize': False\n",
    "#     }, \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'maxvit_small_tf_512',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/maxvitfold0mix_maxvit_small_tf_512_best_epochstage2oof685cv657-00.bin',\n",
    "#         'threshold': 0.62, #0.24,\n",
    "#         'call_sign': \"nir_01_tta\",\n",
    "#         'model_func': load_model4,\n",
    "#         'tta': True,\n",
    "#         'normalize': False\n",
    "#     },\n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'eca_nfnet_l1',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/ecanfnetl1v2fold0_eca_nfnet_l1_best_epochstage2-00.bin',\n",
    "#         'threshold': 0.62, #0.24,\n",
    "#         'call_sign': \"nir_02_tta\",\n",
    "#         'model_func': load_model2,\n",
    "#         'tta': True,\n",
    "#         'normalize': False\n",
    "#     },\n",
    "#     {\n",
    "#         'model_name': 'Unet++',\n",
    "#         'backbone': 'tf_efficientnet_b8',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/effb8modelv2fold0_tf_efficientnet_b8_best_epochstage2-00.bin',\n",
    "#         'threshold': 0.79,\n",
    "#         'call_sign': \"nir_03_tta\", \n",
    "#         'model_func': load_model3,\n",
    "#         'tta': True,\n",
    "#         'normalize': False\n",
    "#     },\n",
    "#     {\n",
    "#         'model_name': 'Unet++',\n",
    "#         'backbone': 'tf_efficientnet_b7_ns',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/contrail_01/output/nirjhar/qishentrialv2fpn512_tf_efficientnet_b7_ns_best_epochcv650lb675-00.bin',\n",
    "#         'threshold': 0.62, #0.24,\n",
    "#         'call_sign': \"nir_04_tta\", \n",
    "#         'model_func': load_model1,\n",
    "#         'tta': True,\n",
    "#         'normalize': False\n",
    "#     }, \n",
    "    {\n",
    "        'model_name': 'Unet',\n",
    "        'backbone': 'gluon_senet154',\n",
    "        'img_size': [512, 512],\n",
    "        'num_classes': 1,\n",
    "        'threshold': 0.5, #0.24,\n",
    "        'call_sign': \"ioa_01\",\n",
    "        'model_func': load_model_exp28_snapshot,\n",
    "        'tta': True, #True\n",
    "        'normalize': True,\n",
    "        'model_pth': [\n",
    "            '/home/rohits/pv1/Contrail_Detection/output/ioannis/exp_28/Unet-gluon_senet154_fold-1.ckpt',\n",
    "            '/home/rohits/pv1/Contrail_Detection/output/ioannis/exp_28/Unet-gluon_senet154_fold-1-v1.ckpt',\n",
    "            '/home/rohits/pv1/Contrail_Detection/output/ioannis/exp_28/Unet-gluon_senet154_fold-1-v2.ckpt',\n",
    "        ]\n",
    "    }, \n",
    "    {\n",
    "        'model_name': 'Unet',\n",
    "        'backbone': 'gluon_senet154',\n",
    "        'img_size': [512, 512],\n",
    "        'num_classes': 1,\n",
    "        'threshold': 0.5, #0.24,\n",
    "        'model_func': load_model_exp28,\n",
    "        'model_pth': '/home/rohits/pv1/Contrail_Detection/output/ioannis/exp_28/Unet-gluon_senet154_fold0.ckpt',        \n",
    "        'call_sign': \"ioa_02\",\n",
    "        'tta': True, #False, #True\n",
    "        'normalize': True\n",
    "    }\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a10c74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dc74b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "646ce6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'gluon_senet154', 'img_size': [512, 512], 'num_classes': 1, 'threshold': 0.5, 'call_sign': 'ioa_01', 'model_func': <function load_model_exp28_snapshot at 0x7fdee0e22cb0>, 'tta': True, 'normalize': True, 'model_pth': ['/home/rohits/pv1/Contrail_Detection/output/ioannis/exp_28/Unet-gluon_senet154_fold-1.ckpt', '/home/rohits/pv1/Contrail_Detection/output/ioannis/exp_28/Unet-gluon_senet154_fold-1-v1.ckpt', '/home/rohits/pv1/Contrail_Detection/output/ioannis/exp_28/Unet-gluon_senet154_fold-1-v2.ckpt']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.10 to v2.0.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../output/ioannis/exp_28/Unet-gluon_senet154_fold-1.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights from: /home/rohits/pv1/Contrail_Detection/output/ioannis/exp_28/Unet-gluon_senet154_fold-1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.10 to v2.0.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../output/ioannis/exp_28/Unet-gluon_senet154_fold-1-v1.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights from: /home/rohits/pv1/Contrail_Detection/output/ioannis/exp_28/Unet-gluon_senet154_fold-1-v1.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.10 to v2.0.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../output/ioannis/exp_28/Unet-gluon_senet154_fold-1-v2.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights from: /home/rohits/pv1/Contrail_Detection/output/ioannis/exp_28/Unet-gluon_senet154_fold-1-v2.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a63f06525d4d79ab3e5deb177a2930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'gluon_senet154', 'img_size': [512, 512], 'num_classes': 1, 'threshold': 0.5, 'model_func': <function load_model_exp28 at 0x7fdee0e22b00>, 'model_pth': '/home/rohits/pv1/Contrail_Detection/output/ioannis/exp_28/Unet-gluon_senet154_fold0.ckpt', 'call_sign': 'ioa_02', 'tta': True, 'normalize': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.4.8 to v2.0.4. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../output/ioannis/exp_28/Unet-gluon_senet154_fold0.ckpt`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08a10bd7cff94028bafaa3f6f6bb0995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_preds = []\n",
    "\n",
    "for idx, cfg in enumerate(CFGS1):   \n",
    "\n",
    "#     if idx <= 7:\n",
    "#         continue\n",
    "        \n",
    "    print(cfg)\n",
    "    val_transform = get_transform(cfg['img_size'])\n",
    "    val_transform2 = get_transform2(cfg['img_size'])\n",
    "\n",
    "    valid_dataset = ContrailDataset(val_df, transform=val_transform, normalize=False)  \n",
    "    if cfg['normalize'] and (\"ioa_\" in cfg['call_sign']):\n",
    "        valid_dataset = ContrailDataset(val_df, transform=val_transform2, normalize=True)  \n",
    "#         valid_dataset = ContrailDataset(val_df, transform=val_transform, normalize=True)  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = 32, #32, \n",
    "        shuffle = False, \n",
    "        num_workers = 4, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model_base = cfg['model_func'](cfg)        \n",
    "      \n",
    "    if cfg['tta']:\n",
    "        if \"roh_\" in cfg['call_sign']:\n",
    "            model = tta.SegmentationTTAWrapper(model_base, tta.aliases.flip_transform(), merge_mode='mean')\n",
    "        else:\n",
    "            model = tta.SegmentationTTAWrapper(model_base, tta.aliases.hflip_transform(), merge_mode='mean')\n",
    "    \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    \n",
    "    \n",
    "    model_base.to(device)\n",
    "    model_base.eval()\n",
    "    \n",
    "\n",
    "    \n",
    "    preds = []\n",
    "#     masks_ = []      \n",
    "        \n",
    "#     for index, (images, masks) in enumerate(tqdm(valid_loader)):  \n",
    "    for index, (images) in enumerate(tqdm(valid_loader)):  \n",
    "\n",
    "        images  = images.to(device, dtype=torch.float)\n",
    "#         masks  = masks.to(device, dtype=torch.float)\n",
    "#         if cfg['img_size'][0] != 256:\n",
    "#             masks = torch.nn.functional.interpolate(masks, size=256, mode='nearest') \n",
    "#         masks_.append(torch.squeeze(masks, dim=1))\n",
    "        with torch.inference_mode():\n",
    "            images = torch.nn.functional.interpolate(images, size=cfg['img_size'][0], mode='nearest')\n",
    "            pred = model_base(images).sigmoid()   \n",
    "            if cfg['tta']:\n",
    "                pred2 = model(images).sigmoid()\n",
    "                pred = (pred + pred2) / 2\n",
    "                \n",
    "            pred = torch.nn.functional.interpolate(pred, size=256, mode='nearest')\n",
    "            preds.append(torch.squeeze(pred, dim=1))\n",
    "        \n",
    "            \n",
    "            \n",
    "    model_preds = torch.cat(preds, dim=0).detach().cpu()  \n",
    "    torch.save(model_preds, f\"../../output/pseudo_preds/{cfg['call_sign']}.pt\")    \n",
    "\n",
    "    \n",
    "    \n",
    "#     model_masks = torch.cat(masks_, dim=0)\n",
    "#     model_preds = torch.cat(preds, dim=0)\n",
    "        \n",
    "#     model_masks = torch.flatten(model_masks, start_dim=0, end_dim=1)\n",
    "#     model_preds = torch.flatten(model_preds, start_dim=0, end_dim=1)  \n",
    "    \n",
    "#     # save\n",
    "#     torch.save(model_preds, f\"../../output/final_preds/{cfg['call_sign']}.pt\")    \n",
    "    \n",
    "#     best_threshold = 0.0\n",
    "#     best_dice_score = 0.0\n",
    "#     for threshold in [i / 100 for i in range(101)] :\n",
    "#         score = dice_coef(model_masks, model_preds, thr=threshold).cpu().detach().numpy() \n",
    "\n",
    "        \n",
    "#         if score > best_dice_score:\n",
    "#             best_dice_score = score\n",
    "#             best_threshold = threshold\n",
    "    \n",
    "        \n",
    "#     print(best_dice_score, best_threshold)\n",
    "#     final_preds1.append(model_preds)\n",
    "    \n",
    "    \n",
    "    final_preds.append(model_preds)\n",
    "    \n",
    "    if cfg['tta']: del model\n",
    "    del model_base\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b0219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfe956de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "392c4823a8554911a83394d4fe0963cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "call_signs = [\n",
    "    \"roh_01_tta\", \"nir_01_tta\", \"nir_02_tta\", \"nir_03_tta\", \"nir_04_tta\", \"ioa_01\", \"ioa_02\"\n",
    "]\n",
    "\n",
    "\n",
    "final_preds = []\n",
    "for sign in tqdm(call_signs, total=len(call_signs)):\n",
    "    wt = torch.load(f\"/home/rohits/pv1/Contrail_Detection/output/pseudo_preds/{sign}.pt\") \n",
    "    final_preds.append(wt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4605a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d231b1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = torch.stack(final_preds).mean(dim=0)\n",
    "final_preds = (final_preds>0.35).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1667e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df['id'] = val_df['image'].apply(lambda x: x.split(\"/\")[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eff5a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4643cc4fa44bae900db410f879f030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20529 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = val_df['id'].values\n",
    "for (val, label_id) in tqdm(zip(final_preds, ids), total=len(ids)): \n",
    "    mask = val.view(256, 256, 1).detach().cpu().numpy()\n",
    "    np.save(f\"../../input/pseudo/{folder}_data_{NTB}/{label_id}/label_6811_702lb.npy\", mask.astype('float16')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950cbca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c53ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb905112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17dcfd3543cb4d638b4471fef353fae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6510141\n",
      "0.66153294\n",
      "0.649828\n",
      "0.6697093\n",
      "0.6561677\n",
      "0.66287154\n",
      "0.6537375\n",
      "0.5 TH Score:  0.6811164\n",
      "0.6867434 0.35\n"
     ]
    }
   ],
   "source": [
    "# 0.657336 0.01\n",
    "\n",
    "\n",
    "# call_signs1 = [\n",
    "#     \"roh_01_tta\", \"nir_01_tta\", \"nir_02_tta\", \"nir_03_tta\", \"nir_04_tta\"\n",
    "# ] \n",
    "\n",
    "# call_signs1 = [\n",
    "#     \"roh_02_tta\",  \"roh_03_tta\", \"roh_04_tta\", \"roh_05_tta\", \"roh_06_tta\",  \"roh_07_tta\",  \"roh_08_tta\"  \n",
    "# ] \n",
    "\n",
    "call_signs1 = [\n",
    "    \"roh_01_tta\", \n",
    "    \"nir_01_tta\",\n",
    "    \"nir_02_tta\",\n",
    "    \"nir_03_tta\",\n",
    "    \"nir_04_tta\",\n",
    "    \n",
    "    \"ioa_01\", \n",
    "    \"ioa_02\",\n",
    "    \n",
    "] \n",
    "\n",
    "model_masks = torch.load(f\"/home/rohits/pv1/Contrail_Detection/output/final_preds/val_masks.pt\")\n",
    "\n",
    "\n",
    "preds1 = []\n",
    "\n",
    "for idx, sign in tqdm(enumerate(call_signs1), total=len(call_signs1)):\n",
    "    wt = torch.load(f\"/home/rohits/pv1/Contrail_Detection/output/final_preds/{sign}.pt\")    \n",
    "    preds1.append(wt)\n",
    "    \n",
    "    score = dice_coef(model_masks, wt, thr=0.5).cpu().detach().numpy() \n",
    "    print(score)\n",
    "    \n",
    "    \n",
    "# preds1.append(preds2)\n",
    "    \n",
    "final_preds = preds1\n",
    "final_preds = torch.stack(final_preds).mean(dim=0)\n",
    "score = dice_coef(model_masks, final_preds, thr=0.5).cpu().detach().numpy() \n",
    "\n",
    "print(\"0.5 TH Score: \", score)\n",
    "\n",
    "\n",
    "best_threshold = 0.0\n",
    "best_dice_score = 0.0\n",
    "for threshold in [i / 100 for i in range(101)] :\n",
    "    score = dice_coef(model_masks, final_preds, thr=threshold).cpu().detach().numpy() \n",
    "    if score > best_dice_score:\n",
    "        best_dice_score = score\n",
    "        best_threshold = threshold\n",
    "print(best_dice_score, best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73749c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5 TH Score:  0.67887855\n",
    "# 0.6856482 0.28\n",
    "\n",
    "\n",
    "\n",
    "# 0.5 TH Score:  0.6811164\n",
    "# 0.6867434 0.35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a82ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
