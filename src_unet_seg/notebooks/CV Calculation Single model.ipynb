{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c125d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ttach as tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "645a79e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna\n",
    "# !pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77e5af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import cv2\n",
    "from torch.cuda import amp\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torchvision.utils import make_grid\n",
    "import optuna\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "157ff8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12e6bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5654e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "CFGS = [           \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'efficientnet-b7',\n",
    "#         'img_size': [256, 256],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth': '../../output/exp_pl2_new01/Unet/efficientnet-b7-256/checkpoint_dice.pth',\n",
    "#         'threshold': 0.84,\n",
    "#         'tta': True, \n",
    "#     },\n",
    "    {\n",
    "        'model_name': 'Unet',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'img_size': [384, 384],\n",
    "        'num_classes': 1,\n",
    "        'model_pth': '/home/rohits/pv1/Contrail_Detection/output/exp_01_pl_687lb/Unet/efficientnet-b7-384/checkpoint_dice_ctrl_fold0.pth',\n",
    "        'threshold': 0.9,\n",
    "        'tta': True, \n",
    "    },\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ddaafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d58a5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrailDataset:\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df  \n",
    "        self.images = df['image']\n",
    "        self.labels = df['label']\n",
    "        self.transform =transform\n",
    "#         self.tta = tta_count\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(\"../../input/\" + self.images[idx]).astype(float)   \n",
    "        label = np.load(\"../../input/\" + self.labels[idx]).astype(float)\n",
    "        \n",
    "        \n",
    "        if self.transform :\n",
    "            data = self.transform(image=image, mask=label)\n",
    "            image  = data['image']\n",
    "            label  = data['mask']\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            label = np.transpose(label, (2, 0, 1))    \n",
    "            \n",
    "#         if  self.transform :\n",
    "#             test_aug = self.transform(image=image, mask=label)\n",
    "#             augmented_img  = test_aug['image']\n",
    "#             label  = test_aug['mask']\n",
    "            \n",
    "#             if self.tta > 1:\n",
    "#                 images = []\n",
    "#                 images.append(augmented_img)\n",
    "#                 images.append(self.transform(image=np.fliplr(image))['image'])\n",
    "#                 if self.tta > 2:\n",
    "#                     images.append(self.transform(image=np.flipud(image))['image'])\n",
    "#                 if self.tta > 3:\n",
    "#                     images.append(self.transform(image=np.flipud(np.fliplr(image)))['image'])\n",
    "#                 image = np.stack(images, axis=0)\n",
    "# #                 print(image.shape )\n",
    "#                 image = np.transpose(image, (0,3,1,2))\n",
    "# #                 print(image.shape)\n",
    "# #                 print(\"*\"*100)\n",
    "#             else:\n",
    "#                 image = augmented_img\n",
    "#                 image = np.transpose(image, (2, 0, 1))\n",
    "                        \n",
    "#             label = np.transpose(label, (2, 0, 1))\n",
    "            \n",
    "        return torch.tensor(image), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "985b3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=cfg[\"backbone\"],      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "            encoder_weights=None,     # use `imagenet` pre-trained weights for encoder initialization\n",
    "            in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "            classes=cfg[\"num_classes\"],        # model output channels (number of classes in your dataset)\n",
    "            activation=None\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        mask = self.model(inputs)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b13dbb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Dice(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, pred_masks: torch.Tensor, gt_masks: torch.Tensor, smooth=1e-5):\n",
    "        \"\"\"\n",
    "        This method expect boolean pred and gt masks in the shape of (B, C, H, W).\n",
    "        In this competition, we are dealing with semantic segmentation and I choose to use \n",
    "        one channel with two values: 0 for background, 1 for mask.\n",
    "        \"\"\"\n",
    "        batch_size = pred_masks.shape[0]\n",
    "        pred_masks = pred_masks.view(batch_size, -1)\n",
    "        gt_masks = gt_masks.view(batch_size, -1)\n",
    "\n",
    "        intersection = (pred_masks * gt_masks).sum(1)\n",
    "        dice = (2. * intersection) / (pred_masks.sum(1) + gt_masks.sum(1) + smooth)\n",
    "\n",
    "        dice = dice.mean()\n",
    "\n",
    "        return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07bfce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, thr=0.5, epsilon=1e-6):\n",
    "    y_true = y_true.to(torch.float32)\n",
    "    y_pred = (y_pred>thr).to(torch.float32)\n",
    "    inter = (y_true*y_pred).sum()\n",
    "    den = y_true.sum() + y_pred.sum()\n",
    "    dice = ((2*inter+epsilon)/(den+epsilon)).mean()\n",
    "    \n",
    "    return dice\n",
    "\n",
    "\n",
    "\n",
    "def get_transform(img_size):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(*img_size, interpolation=cv2.INTER_NEAREST),\n",
    "    ], p=1.0)\n",
    "    return transform\n",
    "\n",
    "\n",
    "\n",
    "def post_process_minsize(mask, min_size=10):\n",
    "    '''Post processing of each predicted mask, components with lesser number of pixels\n",
    "    than `min_size` are ignored'''\n",
    "    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n",
    "    predictions = np.zeros(mask.shape, np.float32)\n",
    "    num = 0\n",
    "    for c in range(1, num_component):\n",
    "        p = (component == c)\n",
    "        if p.sum() > min_size:\n",
    "            predictions[p] = 1\n",
    "            num += 1\n",
    "    return predictions, num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "646ce6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'efficientnet-b7', 'img_size': [384, 384], 'num_classes': 1, 'model_pth': '/home/rohits/pv1/Contrail_Detection/output/exp_01_pl_687lb/Unet/efficientnet-b7-384/checkpoint_dice_ctrl_fold0.pth', 'threshold': 0.9, 'tta': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f040a0a3c148b8bb0de0f2bf66e2d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6749351 0.33\n"
     ]
    }
   ],
   "source": [
    "val_df = pd.read_csv(\"../../input/data_utils/val_df_filled.csv\")\n",
    "# val_df = val_df.loc[val_df['class'] == 'tensor(1)'].reset_index(drop=True)\n",
    "# train_df = pd.read_csv(\"../../input/data_utils/train_5_folds.csv\")\n",
    "# val_df = train_df.loc[train_df.fold == 0].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "final_preds = []\n",
    "\n",
    "for idx, cfg in enumerate(CFGS):\n",
    "    print(cfg)\n",
    "    val_transform = get_transform(cfg['img_size'])\n",
    "    valid_dataset = ContrailDataset(val_df, transform=val_transform)  \n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = 32, #32, \n",
    "        shuffle = False, \n",
    "        num_workers = 2, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "\n",
    "    model = Net(cfg)    \n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    model.load_state_dict(torch.load(cfg['model_pth'], map_location=torch.device('cpu'))['model'])\n",
    "    if cfg['tta']:\n",
    "        model = tta.SegmentationTTAWrapper(model, tta.aliases.flip_transform(), merge_mode='mean')\n",
    "\n",
    "        \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    preds = []\n",
    "    masks_ = []\n",
    "    \n",
    "    for index, (images, masks) in enumerate(tqdm(valid_loader)):  \n",
    "        images  = images.to(device, dtype=torch.float)\n",
    "        masks  = masks.to(device, dtype=torch.float)\n",
    "        if cfg['img_size'][0] != 256:\n",
    "            masks = torch.nn.functional.interpolate(masks, size=256, mode='nearest')             \n",
    "        masks_.append(torch.squeeze(masks, dim=1))\n",
    "\n",
    "        with torch.inference_mode():            \n",
    "            pred = model(images)                \n",
    "            pred = pred.sigmoid()\n",
    "        \n",
    "            if cfg['img_size'][0] != 256:\n",
    "                pred = torch.nn.functional.interpolate(pred, size=256, mode='nearest') \n",
    "            preds.append(torch.squeeze(pred, dim=1))\n",
    "\n",
    "    \n",
    "#     model_masks = torch.stack(masks_, dim=0)\n",
    "#     model_preds = torch.stack(preds, dim=0)\n",
    "    \n",
    "    model_masks = torch.cat(masks_, dim=0)\n",
    "    model_preds = torch.cat(preds, dim=0)\n",
    "    \n",
    "    model_masks = torch.flatten(model_masks, start_dim=0, end_dim=1)\n",
    "    model_preds = torch.flatten(model_preds, start_dim=0, end_dim=1)  \n",
    "    dice_score = dice_coef(model_masks, model_preds).cpu().detach().numpy() \n",
    "    dice_score_th = dice_coef(model_masks, model_preds, thr=cfg['threshold']).cpu().detach().numpy() \n",
    "        \n",
    "        \n",
    "        \n",
    "    best_threshold = 0.0\n",
    "    best_dice_score = 0.0\n",
    "    for threshold in [i / 100 for i in range(101)] :        \n",
    "        score = dice_coef(model_masks, model_preds, thr=threshold).cpu().detach().numpy() \n",
    "        if score > best_dice_score:\n",
    "            best_dice_score = score\n",
    "            best_threshold = threshold\n",
    "    \n",
    "        \n",
    "    print(best_dice_score, best_threshold)\n",
    "        \n",
    "        \n",
    "#     print(dice_score, dice_score_th)\n",
    "    final_preds.append(model_preds)\n",
    "    \n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # 0.65424037"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e1d72a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boost_dice(params):    \n",
    "    for index, val in enumerate(params.keys()):\n",
    "        if index == 0:\n",
    "            preds = params[val]*final_preds[0]\n",
    "        else:\n",
    "            preds += params[val]*final_preds[index]\n",
    "    \n",
    "    \n",
    "    param_sum = 0\n",
    "    for key, val in params.items():\n",
    "        param_sum += val\n",
    "\n",
    "    preds = preds/param_sum\n",
    "    \n",
    "    \n",
    "#     threshold = 0.2\n",
    "#     preds1 = (nn.Sigmoid()(preds)>threshold).double()            \n",
    "#     score = dice_coef(model_masks, preds1, thr=threshold).cpu().detach().numpy() # get_score(labels, preds) \n",
    "    best_threshold = 0.0\n",
    "    best_score = 0.0\n",
    "    for threshold in [i / 100 for i in range(101)] :\n",
    "#         if threshold < 0.05 and threshold > 0.25:\n",
    "#             continue\n",
    "    \n",
    "        preds1 = (nn.Sigmoid()(preds)>threshold).double()            \n",
    "        score = dice_coef(model_masks, preds1, thr=threshold).cpu().detach().numpy() # get_score(labels, preds) \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = threshold\n",
    "            \n",
    "#     best_score = score\n",
    "#     best_threshold = threshold\n",
    "    return best_score, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {}    \n",
    "    for i in range(len(final_preds)):\n",
    "        params[f\"w{i+1}\"] = trial.suggest_float(f'w{i+1}', 0, 1)  \n",
    "        \n",
    "        \n",
    "    score, best_threshold  = boost_dice(params)\n",
    "    params['threshold'] = best_threshold\n",
    "    print(params)\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "\n",
    "study.optimize(objective, n_trials=1500)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f44e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63a6b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7eed6aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones([32,3,3,256,256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "27a3505e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 3, 256, 256])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "831f24ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256])\n",
      "torch.Size([32, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for a in [x[:, 0,:,:,:], x[:, 1,:,:,:]]:\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccabdc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = torch.mean(torch.stack(pred_tta), dim=0)            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
