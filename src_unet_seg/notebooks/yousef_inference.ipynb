{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "041ac14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "# !pip install --upgrade segmentation_models_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e5af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import gc\n",
    "import pandas as pd\n",
    "import ttach as tta\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import cv2\n",
    "from torch.cuda import amp\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torchvision.utils import make_grid\n",
    "import optuna\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "157ff8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12e6bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb77d81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5654e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ddaafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffba53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.model = smp.Unet(**kwargs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class ConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, norm_layer=nn.BatchNorm2d, bias=False):\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n",
    "                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2),\n",
    "            norm_layer(out_channels),\n",
    "            nn.ReLU6()\n",
    "        )\n",
    "\n",
    "\n",
    "class ConvBN(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, norm_layer=nn.BatchNorm2d, bias=False):\n",
    "        super(ConvBN, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n",
    "                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2),\n",
    "            norm_layer(out_channels)\n",
    "        )\n",
    "\n",
    "\n",
    "class Conv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, stride=1, bias=False):\n",
    "        super(Conv, self).__init__(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, bias=bias,\n",
    "                      dilation=dilation, stride=stride, padding=((stride - 1) + dilation * (kernel_size - 1)) // 2)\n",
    "        )\n",
    "\n",
    "\n",
    "class SeparableConvBNReLU(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1,\n",
    "                 norm_layer=nn.BatchNorm2d):\n",
    "        super(SeparableConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n",
    "                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n",
    "                      groups=in_channels, bias=False),\n",
    "            norm_layer(out_channels),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.ReLU6()\n",
    "        )\n",
    "\n",
    "\n",
    "class SeparableConvBN(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1,\n",
    "                 norm_layer=nn.BatchNorm2d):\n",
    "        super(SeparableConvBN, self).__init__(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n",
    "                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n",
    "                      groups=in_channels, bias=False),\n",
    "            norm_layer(out_channels),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class SeparableConv(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, dilation=1):\n",
    "        super(SeparableConv, self).__init__(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, dilation=dilation,\n",
    "                      padding=((stride - 1) + dilation * (kernel_size - 1)) // 2,\n",
    "                      groups=in_channels, bias=False),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        )\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.ReLU6, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Conv2d(in_features, hidden_features, 1, 1, 0, bias=True)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Conv2d(hidden_features, out_features, 1, 1, 0, bias=True)\n",
    "        self.drop = nn.Dropout(drop, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GlobalLocalAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 dim=256,\n",
    "                 num_heads=16,\n",
    "                 qkv_bias=False,\n",
    "                 window_size=8,\n",
    "                 relative_pos_embedding=True\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // self.num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.ws = window_size\n",
    "\n",
    "        self.qkv = Conv(dim, 3*dim, kernel_size=1, bias=qkv_bias)\n",
    "        self.local1 = ConvBN(dim, dim, kernel_size=3)\n",
    "        self.local2 = ConvBN(dim, dim, kernel_size=1)\n",
    "        self.proj = SeparableConvBN(dim, dim, kernel_size=window_size)\n",
    "\n",
    "        self.attn_x = nn.AvgPool2d(kernel_size=(window_size, 1), stride=1,  padding=(window_size//2 - 1, 0))\n",
    "        self.attn_y = nn.AvgPool2d(kernel_size=(1, window_size), stride=1, padding=(0, window_size//2 - 1))\n",
    "\n",
    "        self.relative_pos_embedding = relative_pos_embedding\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            # define a parameter table of relative position bias\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n",
    "\n",
    "            # get pair-wise relative position index for each token inside the window\n",
    "            coords_h = torch.arange(self.ws)\n",
    "            coords_w = torch.arange(self.ws)\n",
    "            coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n",
    "            coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "            relative_coords[:, :, 0] += self.ws - 1  # shift to start from 0\n",
    "            relative_coords[:, :, 1] += self.ws - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.ws - 1\n",
    "            relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "            self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "            trunc_normal_(self.relative_position_bias_table, std=.02)\n",
    "\n",
    "    def pad(self, x, ps):\n",
    "        _, _, H, W = x.size()\n",
    "        if W % ps != 0:\n",
    "            x = F.pad(x, (0, ps - W % ps), mode='reflect')\n",
    "        if H % ps != 0:\n",
    "            x = F.pad(x, (0, 0, 0, ps - H % ps), mode='reflect')\n",
    "        return x\n",
    "\n",
    "    def pad_out(self, x):\n",
    "        x = F.pad(x, pad=(0, 1, 0, 1), mode='reflect')\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        local = self.local2(x) + self.local1(x)\n",
    "\n",
    "        x = self.pad(x, self.ws)\n",
    "        B, C, Hp, Wp = x.shape\n",
    "        qkv = self.qkv(x)\n",
    "\n",
    "        q, k, v = rearrange(qkv, 'b (qkv h d) (hh ws1) (ww ws2) -> qkv (b hh ww) h (ws1 ws2) d', h=self.num_heads,\n",
    "                            d=C//self.num_heads, hh=Hp//self.ws, ww=Wp//self.ws, qkv=3, ws1=self.ws, ws2=self.ws)\n",
    "\n",
    "        dots = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if self.relative_pos_embedding:\n",
    "            relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "                self.ws * self.ws, self.ws * self.ws, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "            relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "            dots += relative_position_bias.unsqueeze(0)\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "        attn = attn @ v\n",
    "\n",
    "        attn = rearrange(attn, '(b hh ww) h (ws1 ws2) d -> b (h d) (hh ws1) (ww ws2)', h=self.num_heads,\n",
    "                         d=C//self.num_heads, hh=Hp//self.ws, ww=Wp//self.ws, ws1=self.ws, ws2=self.ws)\n",
    "\n",
    "        attn = attn[:, :, :H, :W]\n",
    "\n",
    "        out = self.attn_x(F.pad(attn, pad=(0, 0, 0, 1), mode='reflect')) + \\\n",
    "              self.attn_y(F.pad(attn, pad=(0, 1, 0, 0), mode='reflect'))\n",
    "\n",
    "        out = out + local\n",
    "        out = self.pad_out(out)\n",
    "        out = self.proj(out)\n",
    "        # print(out.size())\n",
    "        out = out[:, :, :H, :W]\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim=256, num_heads=16,  mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.ReLU6, norm_layer=nn.BatchNorm2d, window_size=8):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = GlobalLocalAttention(dim, num_heads=num_heads, qkv_bias=qkv_bias, window_size=window_size)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, out_features=dim, act_layer=act_layer, drop=drop)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class WF(nn.Module):\n",
    "    def __init__(self, in_channels=128, decode_channels=128, eps=1e-8):\n",
    "        super(WF, self).__init__()\n",
    "        self.pre_conv = Conv(in_channels, decode_channels, kernel_size=1)\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n",
    "        self.eps = eps\n",
    "        self.post_conv = ConvBNReLU(decode_channels, decode_channels, kernel_size=3)\n",
    "\n",
    "    def forward(self, x, res):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        weights = nn.ReLU()(self.weights)\n",
    "        fuse_weights = weights / (torch.sum(weights, dim=0) + self.eps)\n",
    "        x = fuse_weights[0] * self.pre_conv(res) + fuse_weights[1] * x\n",
    "        x = self.post_conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatureRefinementHead(nn.Module):\n",
    "    def __init__(self, in_channels=64, decode_channels=64):\n",
    "        super().__init__()\n",
    "        self.pre_conv = Conv(in_channels, decode_channels, kernel_size=1)\n",
    "\n",
    "        self.weights = nn.Parameter(torch.ones(2, dtype=torch.float32), requires_grad=True)\n",
    "        self.eps = 1e-8\n",
    "        self.post_conv = ConvBNReLU(decode_channels, decode_channels, kernel_size=3)\n",
    "\n",
    "        self.pa = nn.Sequential(nn.Conv2d(decode_channels, decode_channels, kernel_size=3, padding=1, groups=decode_channels),\n",
    "                                nn.Sigmoid())\n",
    "        self.ca = nn.Sequential(nn.AdaptiveAvgPool2d(1),\n",
    "                                Conv(decode_channels, decode_channels//16, kernel_size=1),\n",
    "                                nn.ReLU6(),\n",
    "                                Conv(decode_channels//16, decode_channels, kernel_size=1),\n",
    "                                nn.Sigmoid())\n",
    "\n",
    "        self.shortcut = ConvBN(decode_channels, decode_channels, kernel_size=1)\n",
    "        self.proj = SeparableConvBN(decode_channels, decode_channels, kernel_size=3)\n",
    "        self.act = nn.ReLU6()\n",
    "\n",
    "    def forward(self, x, res):\n",
    "        x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n",
    "        weights = nn.ReLU()(self.weights)\n",
    "        fuse_weights = weights / (torch.sum(weights, dim=0) + self.eps)\n",
    "        x = fuse_weights[0] * self.pre_conv(res) + fuse_weights[1] * x\n",
    "        x = self.post_conv(x)\n",
    "        shortcut = self.shortcut(x)\n",
    "        pa = self.pa(x) * x\n",
    "        ca = self.ca(x) * x\n",
    "        x = pa + ca\n",
    "        x = self.proj(x) + shortcut\n",
    "        x = self.act(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class AuxHead(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels=64, num_classes=8):\n",
    "        super().__init__()\n",
    "        self.conv = ConvBNReLU(in_channels, in_channels)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.conv_out = Conv(in_channels, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, h, w):\n",
    "        feat = self.conv(x)\n",
    "        feat = self.drop(feat)\n",
    "        feat = self.conv_out(feat)\n",
    "        feat = F.interpolate(feat, size=(h, w), mode='bilinear', align_corners=False)\n",
    "        return feat\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 encoder_channels=(64, 128, 256, 512),\n",
    "                 decode_channels=64,\n",
    "                 dropout=0.1,\n",
    "                 window_size=8,\n",
    "                 num_classes=6):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.pre_conv = ConvBN(encoder_channels[-1], decode_channels, kernel_size=1)\n",
    "        self.b4 = Block(dim=decode_channels, num_heads=8, window_size=window_size)\n",
    "\n",
    "        self.b3 = Block(dim=decode_channels, num_heads=8, window_size=window_size)\n",
    "        self.p3 = WF(encoder_channels[-2], decode_channels)\n",
    "\n",
    "        self.b2 = Block(dim=decode_channels, num_heads=8, window_size=window_size)\n",
    "        self.p2 = WF(encoder_channels[-3], decode_channels)\n",
    "\n",
    "        if self.training:\n",
    "            self.up4 = nn.UpsamplingBilinear2d(scale_factor=4)\n",
    "            self.up3 = nn.UpsamplingBilinear2d(scale_factor=2)\n",
    "            self.aux_head = AuxHead(decode_channels, num_classes)\n",
    "\n",
    "        self.p1 = FeatureRefinementHead(encoder_channels[-4], decode_channels)\n",
    "\n",
    "        self.segmentation_head = nn.Sequential(ConvBNReLU(decode_channels, decode_channels),\n",
    "                                               nn.Dropout2d(p=dropout, inplace=True),\n",
    "                                               Conv(decode_channels, num_classes, kernel_size=1))\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, res1, res2, res3, res4, h, w):\n",
    "        if self.training:\n",
    "            x = self.b4(self.pre_conv(res4))\n",
    "            h4 = self.up4(x)\n",
    "\n",
    "            x = self.p3(x, res3)\n",
    "            x = self.b3(x)\n",
    "            h3 = self.up3(x)\n",
    "\n",
    "            x = self.p2(x, res2)\n",
    "            x = self.b2(x)\n",
    "            h2 = x\n",
    "            x = self.p1(x, res1)\n",
    "            x = self.segmentation_head(x)\n",
    "            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=False)\n",
    "\n",
    "            ah = h4 + h3 + h2\n",
    "            ah = self.aux_head(ah, h, w)\n",
    "\n",
    "            return x, ah\n",
    "\n",
    "        else:\n",
    "            x = self.b4(self.pre_conv(res4))\n",
    "            x = self.p3(x, res3)\n",
    "            x = self.b3(x)\n",
    "\n",
    "            x = self.p2(x, res2)\n",
    "            x = self.b2(x)\n",
    "\n",
    "            x = self.p1(x, res1)\n",
    "\n",
    "            x = self.segmentation_head(x)\n",
    "            x = F.interpolate(x, size=(h, w), mode='bilinear', align_corners=False)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def init_weight(self):\n",
    "        for m in self.children():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, a=1)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class UNetFormer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 decode_channels=64,\n",
    "                 dropout=0.1,\n",
    "                 backbone_name='swsl_resnet18',\n",
    "                 pretrained=True,\n",
    "                 window_size=8,\n",
    "                 num_classes=6\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        if 'convnext' in backbone_name:\n",
    "            self.backbone = timm.create_model(backbone_name, features_only=True,\n",
    "                                              output_stride=32, pretrained=pretrained)\n",
    "        else:\n",
    "            self.backbone = timm.create_model(backbone_name, features_only=True, output_stride=32,\n",
    "                                              out_indices=(1, 2, 3, 4), pretrained=pretrained)\n",
    "\n",
    "        encoder_channels = self.backbone.feature_info.channels()\n",
    "\n",
    "        self.decoder = Decoder(encoder_channels, decode_channels, dropout, window_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, w = x.size()[-2:]\n",
    "        res1, res2, res3, res4 = self.backbone(x)\n",
    "        if self.training:\n",
    "            x, ah = self.decoder(res1, res2, res3, res4, h, w)\n",
    "            return x, ah\n",
    "        else:\n",
    "            x = self.decoder(res1, res2, res3, res4, h, w)\n",
    "            return x\n",
    "    \n",
    "    \n",
    "def get_model(config):\n",
    "    model = globals().get(config.model.arch)\n",
    "\n",
    "    return model(**config.model.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78b17a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d5fc29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d58a5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# class ContrailDataset:\n",
    "#     def __init__(self, df, transform=None, normalize=False):\n",
    "#         self.df = df  \n",
    "#         self.images = df['image']\n",
    "#         self.labels = df['label']\n",
    "#         self.transform =transform\n",
    "#         self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "#         self.normalize=normalize\n",
    "        \n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.images)\n",
    "        \n",
    "#     def __getitem__(self, idx):\n",
    "#         image = np.load(\"../../input/\" + self.images[idx]).astype(float)   \n",
    "#         label = np.load(\"../../input/\" + self.labels[idx]).astype(float)\n",
    "        \n",
    "\n",
    "#         # label_cls = 1 if label.sum() > 0 else 0\n",
    "#         if self.transform :\n",
    "#             data = self.transform(image=image, mask=label)\n",
    "#             image  = data['image']\n",
    "#             label  = data['mask']\n",
    "#             image = np.transpose(image, (2, 0, 1))\n",
    "#             label = np.transpose(label, (2, 0, 1))    \n",
    "            \n",
    "            \n",
    "# #         return torch.tensor(image), torch.tensor(label)\n",
    "    \n",
    "#         if self.normalize:\n",
    "#             image = self.normalize_image(torch.tensor(image))\n",
    "#             return image, torch.tensor(label)\n",
    "#         else:\n",
    "#             return torch.tensor(image), torch.tensor(label)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "07bfce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred, thr=0.5, epsilon=1e-6):\n",
    "    y_true = y_true.to(torch.float32)\n",
    "    y_pred = (y_pred>thr).to(torch.float32)\n",
    "    inter = (y_true*y_pred).sum()\n",
    "    den = y_true.sum() + y_pred.sum()\n",
    "    dice = ((2*inter+epsilon)/(den+epsilon)).mean()\n",
    "    \n",
    "    return dice\n",
    "\n",
    "\n",
    "def get_transform(img_size):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(*img_size, interpolation=cv2.INTER_NEAREST),\n",
    "    ], p=1.0)\n",
    "    return transform\n",
    "\n",
    "def get_transform2(img_size):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(*img_size, interpolation=cv2.INTER_LINEAR),\n",
    "    ], p=1.0)\n",
    "    return transform\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01378f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(\"../../input/data_utils/val_df_filled.csv\")\n",
    "val_df_full = val_df.copy()\n",
    "\n",
    "val_dups = np.load(\"../../input/data_utils/dups_val.npy\")\n",
    "\n",
    "val_dups = [int(val_id) for val_id in val_dups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aea14014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1856, 4) (1856, 4)\n"
     ]
    }
   ],
   "source": [
    "# val_df = val_df.loc[~val_df['id'].isin(val_dups)].reset_index(drop=True)\n",
    "print(val_df.shape, val_df_full.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "de08c2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install einops\n",
    "# !pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29a963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c51a2f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import functional as transforms_F\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, DEFAULT_CROP_PCT\n",
    "from torchvision.utils import _log_api_usage_once\n",
    "\n",
    "\n",
    "_pil_interpolation_to_str = {\n",
    "    Image.NEAREST: 'PIL.Image.NEAREST',\n",
    "    Image.BILINEAR: 'PIL.Image.BILINEAR',\n",
    "    Image.BICUBIC: 'PIL.Image.BICUBIC',\n",
    "    Image.LANCZOS: 'PIL.Image.LANCZOS',\n",
    "    Image.HAMMING: 'PIL.Image.HAMMING',\n",
    "    Image.BOX: 'PIL.Image.BOX',\n",
    "}\n",
    "\n",
    "\n",
    "def _pil_interp(method):\n",
    "    if method == 'bicubic':\n",
    "        return Image.BICUBIC\n",
    "    elif method == 'lanczos':\n",
    "        return Image.LANCZOS\n",
    "    elif method == 'hamming':\n",
    "        return Image.HAMMING\n",
    "    else:\n",
    "        # default bilinear, do we want to allow nearest?\n",
    "        return Image.BILINEAR\n",
    "\n",
    "\n",
    "class Resize(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, size, interpolation=Image.BILINEAR, max_size=None, antialias=None):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        if not isinstance(size, (int, Sequence)):\n",
    "            raise TypeError(f\"Size should be int or sequence. Got {type(size)}\")\n",
    "        if isinstance(size, Sequence) and len(size) not in (1, 2):\n",
    "            raise ValueError(\"If size is a sequence, it should have 1 or 2 values\")\n",
    "        self.size = size\n",
    "        self.max_size = max_size\n",
    "\n",
    "        # Backward compatibility with integer value\n",
    "        if isinstance(interpolation, int):\n",
    "            interpolation = _interpolation_modes_from_int(interpolation)\n",
    "\n",
    "        self.interpolation = interpolation\n",
    "        self.antialias = antialias\n",
    "\n",
    "    def forward(self, img, mask):\n",
    "        img = transforms_F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)\n",
    "        mask = transforms_F.resize(mask, self.size, Image.NEAREST, self.max_size, self.antialias)\n",
    "        return img, mask\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        detail = f\"(size={self.size}, interpolation={self.interpolation.value}, max_size={self.max_size}, antialias={self.antialias})\"\n",
    "        return f\"{self.__class__.__name__}{detail}\"\n",
    "\n",
    "    \n",
    "\n",
    "class ToTensor:\n",
    "    def __init__(self) -> None:\n",
    "        _log_api_usage_once(self)\n",
    "\n",
    "    def __call__(self, img_pic, mask_pic):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        return transforms_F.to_tensor(img_pic), transforms_F.to_tensor(mask_pic)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "\n",
    "    \n",
    "class Normalize(torch.nn.Module):\n",
    "    def __init__(self, mean, std, inplace=False):\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        self.inplace = inplace\n",
    "\n",
    "    def forward(self, img_tensor, mask_tensor) -> tuple:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Normalized Tensor image.\n",
    "        \"\"\"\n",
    "        return transforms_F.normalize(img_tensor, self.mean, self.std, self.inplace), mask_tensor\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}(mean={self.mean}, std={self.std})\"\n",
    "\n",
    "    \n",
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
    "            _log_api_usage_once(self)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, img, mask):\n",
    "        for t in self.transforms:\n",
    "            img, mask = t(img, mask)\n",
    "        return img, mask\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        format_string = self.__class__.__name__ + \"(\"\n",
    "        for t in self.transforms:\n",
    "            format_string += \"\\n\"\n",
    "            format_string += f\"    {t}\"\n",
    "        format_string += \"\\n)\"\n",
    "        return format_string\n",
    "\n",
    "\n",
    "def transforms_imagenet_eval(\n",
    "        img_size=224,\n",
    "        crop_pct=None,\n",
    "        interpolation='bilinear',\n",
    "#         interpolation='nearest',\n",
    "        use_prefetcher=False,\n",
    "        mean=IMAGENET_DEFAULT_MEAN,\n",
    "        std=IMAGENET_DEFAULT_STD,\n",
    "        normalize=True,\n",
    "):\n",
    "    tfl = [\n",
    "        Resize(img_size, _pil_interp(interpolation)),\n",
    "    ]\n",
    "    if use_prefetcher:\n",
    "        # prefetcher and collate will handle tensor conversion and norm\n",
    "        tfl += [ToNumpy()]\n",
    "    else:\n",
    "        tfl += [\n",
    "            ToTensor(),\n",
    "        ]\n",
    "\n",
    "        if normalize:\n",
    "            tfl += [\n",
    "                Normalize(\n",
    "                 mean=torch.tensor(mean),\n",
    "                 std=torch.tensor(std))\n",
    "            ]\n",
    "\n",
    "    return Compose(tfl)\n",
    "\n",
    "\n",
    "def get_transforms(phase_config):\n",
    "    return transforms_imagenet_eval(\n",
    "            img_size=phase_config.Resize.height,\n",
    "            normalize=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b51b45d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dce9ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrailDataset:\n",
    "    def __init__(self, df, transform=None, normalize=False):\n",
    "        self.df = df  \n",
    "        self.images = df['image']\n",
    "        self.labels = df['label']\n",
    "        self.transform =transform\n",
    "#         self.normalize_image = T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        self.normalize=normalize\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        image = np.load(\"../../input/\" + self.images[idx]).astype(float)   \n",
    "        label = np.load(\"../../input/\" + self.labels[idx]).astype(float)\n",
    "        \n",
    "\n",
    "        # label_cls = 1 if label.sum() > 0 else 0\n",
    "        if self.transform :\n",
    "            data = self.transform(image=image, mask=label)\n",
    "            image  = data['image']\n",
    "            label  = data['mask']\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            label = np.transpose(label, (2, 0, 1))    \n",
    "            \n",
    "            \n",
    "#         return torch.tensor(image), torch.tensor(label)\n",
    "    \n",
    "        if self.normalize:\n",
    "            image = self.normalize_image(torch.tensor(image))\n",
    "            return image, torch.tensor(label)\n",
    "        else:\n",
    "            return torch.tensor(image), torch.tensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "637d739f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from easydict import EasyDict as edict\n",
    "import yaml\n",
    "from typing import Sequence\n",
    "from torchvision.transforms.functional import InterpolationMode, _interpolation_modes_from_int\n",
    "\n",
    "\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as fid:\n",
    "        yaml_config = edict(yaml.load(fid, Loader=yaml.Loader))\n",
    "\n",
    "    return yaml_config\n",
    "\n",
    "config = load_config('/home/rohits/pv1/Contrail_Detection/output/config.yml')\n",
    "\n",
    "\n",
    "if config.model.arch != 'UNetFormer':\n",
    "    config.model.params.encoder_weights = None\n",
    "else:\n",
    "    config.model.params.pretrained = False\n",
    "\n",
    "transforms = get_transforms(config.transforms.test)\n",
    "model = get_model(config)\n",
    "model.cuda()\n",
    "model.load_state_dict(torch.load('/home/rohits/pv1/Contrail_Detection/output/checkpoints/best_model.pth')[\"model_state_dict\"])\n",
    "model.eval()\n",
    "model.float()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c48144f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_preds = []\n",
    "\n",
    "        \n",
    "val_transform = get_transform([512,512])\n",
    "valid_dataset = ContrailDataset(val_df, transform=transforms, normalize=True)  \n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size = 32, #32, \n",
    "    shuffle = False, \n",
    "    num_workers = 2, \n",
    "    pin_memory = True, \n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b71a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_transform([512,512])['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "701f403e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Compose([\n",
       "   Resize(always_apply=False, p=1, height=512, width=512, interpolation=0),\n",
       " ], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}),\n",
       " Compose(\n",
       "     Resize(size=512, interpolation=bilinear, max_size=None, antialias=None)\n",
       "     ToTensor()\n",
       "     Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       " ))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_transform, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62484d6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c7e11f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fa13f013f324560bb3b62cbf1135f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/rohits/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/rohits/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/rohits/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_4162342/1165069015.py\", line 21, in __getitem__\n    data = self.transform(image=image, mask=label)\nTypeError: Compose.__call__() got an unexpected keyword argument 'image'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m preds \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      2\u001b[0m masks_ \u001b[38;5;241m=\u001b[39m []      \n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, (images, masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(valid_loader)):  \n\u001b[1;32m      5\u001b[0m     images  \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[1;32m      6\u001b[0m     masks  \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n",
      "File \u001b[0;32m~/anaconda3/envs/contrail/lib/python3.10/site-packages/tqdm/notebook.py:254\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    253\u001b[0m     it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(tqdm_notebook, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__iter__\u001b[39m()\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m it:\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;66;03m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/contrail/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1345\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1344\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1371\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1371\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mTypeError\u001b[0m: Caught TypeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/rohits/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/rohits/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/rohits/anaconda3/envs/contrail/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_4162342/1165069015.py\", line 21, in __getitem__\n    data = self.transform(image=image, mask=label)\nTypeError: Compose.__call__() got an unexpected keyword argument 'image'\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "masks_ = []      \n",
    "\n",
    "for index, (images, masks) in enumerate(tqdm(valid_loader)):  \n",
    "    images  = images.to(device, dtype=torch.float)\n",
    "    masks  = masks.to(device, dtype=torch.float)\n",
    "    masks = torch.nn.functional.interpolate(masks, size=256, mode='nearest') \n",
    "\n",
    "    masks_.append(torch.squeeze(masks, dim=1))\n",
    "    with torch.inference_mode():\n",
    "        images = torch.nn.functional.interpolate(images, size=512, mode='nearest')\n",
    "        pred = model(images).sigmoid()   \n",
    "\n",
    "        pred = torch.nn.functional.interpolate(pred, size=256, mode='nearest')\n",
    "        preds.append(torch.squeeze(pred, dim=1))\n",
    "        \n",
    "    \n",
    "model_masks = torch.cat(masks_, dim=0)\n",
    "model_preds = torch.cat(preds, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0b4af9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6495943 0.52\n"
     ]
    }
   ],
   "source": [
    "best_threshold = 0.0\n",
    "best_dice_score = 0.0\n",
    "for threshold in [i / 100 for i in range(101)] :\n",
    "    score = dice_coef(model_masks, model_preds, thr=threshold).cpu().detach().numpy() \n",
    "    if score > best_dice_score:\n",
    "        best_dice_score = score\n",
    "        best_threshold = threshold\n",
    "        \n",
    "print(best_dice_score, best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b898b4a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=512, interpolation=bilinear, max_size=None, antialias=None)\n",
       "    ToTensor()\n",
       "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0acc817c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "84dc74b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df = val_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "646ce6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet++', 'backbone': 'tf_efficientnet_b8', 'img_size': [512, 512], 'num_classes': 1, 'model_pth': '/home/rohits/pv1/Contrail_Detection/output/nirjhar/fullfinetune-effb8modelv2fold0_tf_efficientnet_b8_best_epochstage2cv670-00.bin', 'threshold': 0.79, 'call_sign': 'nir_05_tta', 'model_func': <function load_model3 at 0x7f4d2aa97eb0>, 'tta': True, 'normalize': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95d890a93af495492009e6114767ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1856, 256, 256])\n",
      "0.6744085 0.76\n"
     ]
    }
   ],
   "source": [
    "final_preds1 = []\n",
    "\n",
    "for idx, cfg in enumerate(CFGS1):   \n",
    "\n",
    "    if idx <= 7:\n",
    "        continue\n",
    "        \n",
    "    print(cfg)\n",
    "    val_transform = get_transform(cfg['img_size'])\n",
    "    val_transform2 = get_transform2(cfg['img_size'])\n",
    "\n",
    "    valid_dataset = ContrailDataset(val_df, transform=val_transform, normalize=False)  \n",
    "    if cfg['normalize'] and (\"ioa_\" in cfg['call_sign']):\n",
    "        valid_dataset = ContrailDataset(val_df, transform=val_transform2, normalize=True)  \n",
    "#         valid_dataset = ContrailDataset(val_df, transform=val_transform, normalize=True)  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = 32, #32, \n",
    "        shuffle = False, \n",
    "        num_workers = 2, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    model_base = cfg['model_func'](cfg)        \n",
    "      \n",
    "    if cfg['tta']:\n",
    "        if \"roh_\" in cfg['call_sign']:\n",
    "            model = tta.SegmentationTTAWrapper(model_base, tta.aliases.flip_transform(), merge_mode='mean')\n",
    "        else:\n",
    "            model = tta.SegmentationTTAWrapper(model_base, tta.aliases.hflip_transform(), merge_mode='mean')\n",
    "    \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "    \n",
    "    \n",
    "    model_base.to(device)\n",
    "    model_base.eval()\n",
    "    \n",
    "\n",
    "    \n",
    "    preds = []\n",
    "    masks_ = []      \n",
    "        \n",
    "    for index, (images, masks) in enumerate(tqdm(valid_loader)):  \n",
    "        images  = images.to(device, dtype=torch.float)\n",
    "        masks  = masks.to(device, dtype=torch.float)\n",
    "        if cfg['img_size'][0] != 256:\n",
    "#             masks = torch.nn.functional.interpolate(masks, size=256, mode='nearest') \n",
    "            masks = torch.nn.functional.interpolate(masks, size=256, mode='nearest') \n",
    "\n",
    "\n",
    "        masks_.append(torch.squeeze(masks, dim=1))\n",
    "        with torch.inference_mode():\n",
    "            images = torch.nn.functional.interpolate(images, size=cfg['img_size'][0], mode='nearest')\n",
    "            pred = model_base(images).sigmoid()   \n",
    "            if cfg['tta']:\n",
    "                pred = model(images).sigmoid()\n",
    "#                 pred2 = model(images).sigmoid()\n",
    "#                 pred = (pred + pred2) / 2\n",
    "                \n",
    "            pred = torch.nn.functional.interpolate(pred, size=256, mode='nearest')\n",
    "            preds.append(torch.squeeze(pred, dim=1))\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "    \n",
    "    model_masks = torch.cat(masks_, dim=0)\n",
    "    model_preds = torch.cat(preds, dim=0)\n",
    "        \n",
    "    print(model_preds.shape)\n",
    "        \n",
    "#     model_masks = torch.flatten(model_masks, start_dim=0, end_dim=1)\n",
    "#     model_preds = torch.flatten(model_preds, start_dim=0, end_dim=1)  \n",
    "    \n",
    "    # save\n",
    "#     torch.save(model_preds, f\"../../output/final_preds/{cfg['call_sign']}.pt\")    \n",
    "#     torch.save(model_preds, f\"../../output/final_preds/{cfg['call_sign']}_full.pt\")    \n",
    "\n",
    "#     if idx == 6:\n",
    "#     torch.save(model_masks, f'../../output/final_preds/val_masks.pt')\n",
    "\n",
    "    np.save(f\"../../output/final_preds/{cfg['call_sign']}\", model_preds.detach().cpu().numpy())\n",
    "    if idx == 0:\n",
    "        np.save(f\"../../output/final_preds/val_masks\", model_masks.detach().cpu().numpy())\n",
    "\n",
    "    \n",
    "    best_threshold = 0.0\n",
    "    best_dice_score = 0.0\n",
    "    for threshold in [i / 100 for i in range(101)] :\n",
    "        score = dice_coef(model_masks, model_preds, thr=threshold).cpu().detach().numpy() \n",
    "\n",
    "        \n",
    "        if score > best_dice_score:\n",
    "            best_dice_score = score\n",
    "            best_threshold = threshold\n",
    "    \n",
    "        \n",
    "    print(best_dice_score, best_threshold)\n",
    "    final_preds1.append(model_preds)\n",
    "    \n",
    "    if cfg['tta']: del model\n",
    "    del model_base\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb905112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da8b947f5a14436b31d3414b45844ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([121634816])\n",
      "0.67922837\n",
      "torch.Size([121634816])\n",
      "0.6797982\n",
      "torch.Size([121634816])\n",
      "0.6567023\n",
      "torch.Size([121634816])\n",
      "0.67340404\n",
      "torch.Size([121634816])\n",
      "0.6707117\n",
      "torch.Size([121634816])\n",
      "0.67332405\n",
      "torch.Size([121634816])\n",
      "0.65260977\n",
      "torch.Size([121634816])\n",
      "0.6755674\n",
      "torch.Size([121634816])\n",
      "0.6600541\n",
      "0.5 TH Score:  0.69304866\n",
      "0.6936055 0.45\n"
     ]
    }
   ],
   "source": [
    "call_signs1 = [\n",
    "#     \"roh_01_tta\", # can be removed\n",
    "    \"roh_02_tta\",\n",
    "    \n",
    "    \n",
    "#     \"roh_03_tta\",\n",
    "    \"roh_04_tta\", \n",
    "    \n",
    "    \n",
    "    \"nir_01_tta\",\n",
    "#     \"nir_02_tta\",\n",
    "#     \"nir_03_tta\",\n",
    "#     \"nir_04_tta\",\n",
    "    \"nir_05_tta\",\n",
    "    \n",
    "    \n",
    "    \"121\", \n",
    "    \"122\", \n",
    "    \"125\", \n",
    "    \"127\", \n",
    "    \"131\"  \n",
    "] \n",
    "\n",
    "model_masks = torch.from_numpy(np.load(f\"/home/rohits/pv1/Contrail_Detection/output/final_preds/val_masks.npy\")).flatten() \n",
    "\n",
    "preds = []\n",
    "\n",
    "for idx, sign in tqdm(enumerate(call_signs1), total=len(call_signs1)):\n",
    "    wt = torch.from_numpy(np.load(f\"/home/rohits/pv1/Contrail_Detection/output/final_preds/{sign}.npy\")).flatten()   \n",
    "    preds.append(wt)\n",
    "    print(wt.shape)\n",
    "    \n",
    "    score = dice_coef(model_masks, wt, thr=0.5).cpu().detach().numpy() \n",
    "    print(score)\n",
    "    \n",
    "    \n",
    "# # preds1.append(preds2)\n",
    "    \n",
    "final_preds = preds\n",
    "final_preds = torch.stack(final_preds).mean(dim=0)\n",
    "score = dice_coef(model_masks, final_preds, thr=0.5).cpu().detach().numpy() \n",
    "\n",
    "print(\"0.5 TH Score: \", score)\n",
    "\n",
    "\n",
    "best_threshold = 0.0\n",
    "best_dice_score = 0.0\n",
    "for threshold in [i / 100 for i in range(101)] :\n",
    "    if threshold < 0.35 or threshold > 0.51:\n",
    "        continue\n",
    "    \n",
    "    score = dice_coef(model_masks, final_preds, thr=threshold).cpu().detach().numpy() \n",
    "    if score > best_dice_score:\n",
    "        best_dice_score = score\n",
    "        best_threshold = threshold\n",
    "        \n",
    "print(best_dice_score, best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05a64043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ,dkngld\n",
    "\n",
    "# 0.6857662 # 0.68593323 : 0.47 TH\n",
    "# 0.68798006  # 0.6884547 0.45\n",
    "# 0.6903099 # 0.6911206 0.44\n",
    "# 0.6909886 # 0.6928196 0.45\n",
    "\n",
    "\n",
    "\n",
    "# 0.6914439 # 0.69235253 0.44\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c590d14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.657336 0.01\n",
    "\n",
    "\n",
    "# call_signs1 = [\n",
    "#     \"roh_01_tta\", \"nir_01_tta\", \"nir_02_tta\", \"nir_03_tta\", \"nir_04_tta\"\n",
    "# ] \n",
    "\n",
    "# call_signs1 = [\n",
    "#     \"roh_02_tta\",  \"roh_03_tta\", \"roh_04_tta\", \"roh_05_tta\", \"roh_06_tta\",  \"roh_07_tta\",  \"roh_08_tta\"  \n",
    "# ] \n",
    "\n",
    "call_signs1 = [\n",
    "    \"roh_01_tta\",\n",
    "    \"roh_02_tta\", \n",
    "    \"roh_03_tta\", \n",
    "    \"nir_01_tta\",\n",
    "    \"nir_02_tta\",\n",
    "    \"nir_03_tta\",\n",
    "    \"nir_04_tta\",\n",
    "    \"121\", \n",
    "    \"122\",\n",
    "    \"125\", \n",
    "    \"127\",\n",
    "    \"131\"\n",
    "    \n",
    "    \n",
    "#     \"ioa_01\", \n",
    "#     \"ioa_02\",\n",
    "#     \"roh_02_tta\"\n",
    "    \n",
    "] \n",
    "\n",
    "model_masks = torch.load(f\"/home/rohits/pv1/Contrail_Detection/output/final_preds/val_masks.pt\")\n",
    "\n",
    "\n",
    "preds1 = []\n",
    "\n",
    "for idx, sign in tqdm(enumerate(call_signs1), total=len(call_signs1)):\n",
    "    wt = torch.load(f\"/home/rohits/pv1/Contrail_Detection/output/final_preds/{sign}.pt\")    \n",
    "    preds1.append(wt)\n",
    "    \n",
    "    score = dice_coef(model_masks, wt, thr=0.5).cpu().detach().numpy() \n",
    "    print(score)\n",
    "    \n",
    "    \n",
    "# preds1.append(preds2)\n",
    "    \n",
    "final_preds = preds1\n",
    "final_preds = torch.stack(final_preds).mean(dim=0)\n",
    "score = dice_coef(model_masks, final_preds, thr=0.5).cpu().detach().numpy() \n",
    "\n",
    "print(\"0.5 TH Score: \", score)\n",
    "\n",
    "\n",
    "best_threshold = 0.0\n",
    "best_dice_score = 0.0\n",
    "for threshold in [i / 100 for i in range(101)] :\n",
    "    score = dice_coef(model_masks, final_preds, thr=threshold).cpu().detach().numpy() \n",
    "    if score > best_dice_score:\n",
    "        best_dice_score = score\n",
    "        best_threshold = threshold\n",
    "print(best_dice_score, best_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73749c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5 TH Score:  0.67887855\n",
    "# 0.6856482 0.28\n",
    "\n",
    "\n",
    "\n",
    "# 0.5 TH Score:  0.6811164\n",
    "# 0.6867434 0.35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e7539d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a7c5d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6469362\n",
      "0.6567023\n",
      "0.6457618\n",
      "0.6704817\n",
      "0.655622\n",
      "0.5 TH Score:  0.6751985\n",
      "0.6808599 0.35\n"
     ]
    }
   ],
   "source": [
    "call_signs1 = [\n",
    "    \"roh_01_tta_full\", \"nir_01_tta_full\", \"nir_02_tta_full\", \"nir_03_tta_full\", \"nir_04_tta_full\"\n",
    "] \n",
    "\n",
    "model_masks = torch.load(f\"/home/rohits/pv1/Contrail_Detection/output/final_preds/val_masks_full.pt\")\n",
    "\n",
    "\n",
    "preds1 = []\n",
    "\n",
    "for idx, sign in tqdm(enumerate(call_signs1), total=len(call_signs1)):\n",
    "    wt = torch.load(f\"/home/rohits/pv1/Contrail_Detection/output/final_preds/{sign}.pt\")    \n",
    "    preds1.append(wt)\n",
    "    \n",
    "    score = dice_coef(model_masks, wt, thr=0.5).cpu().detach().numpy() \n",
    "    print(score)\n",
    "    \n",
    "final_preds = preds1\n",
    "final_preds = torch.stack(final_preds).mean(dim=0)\n",
    "score = dice_coef(model_masks, final_preds, thr=0.5).cpu().detach().numpy() \n",
    "print(\"0.5 TH Score: \", score)\n",
    "\n",
    "best_threshold = 0.0\n",
    "best_dice_score = 0.0\n",
    "for threshold in [i / 100 for i in range(101)] :\n",
    "    score = dice_coef(model_masks, final_preds, thr=threshold).cpu().detach().numpy() \n",
    "    if score > best_dice_score:\n",
    "        best_dice_score = score\n",
    "        best_threshold = threshold\n",
    "print(best_dice_score, best_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6631c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.5 TH Score:  0.6760312\n",
    "# 0.6810049 0.33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2290774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CFGS1 = [   \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'gluon_senet154',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'threshold': 0.5, #0.24,\n",
    "#         'model_func': load_model_exp28,\n",
    "#         'model_pth': '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1.ckpt',\n",
    "#         'call_sign': \"ioa_01\",\n",
    "#         'tta': True, #False, #True\n",
    "#         'normalize': True\n",
    "#     }, \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'gluon_senet154',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'threshold': 0.5, #0.24,\n",
    "#         'call_sign': \"ioa_02\",\n",
    "#         'model_func': load_model_exp28_snapshot,\n",
    "#         'tta': False, #True\n",
    "#         'normalize': True,\n",
    "#         'model_pth': [\n",
    "#             ######## full train.csv \n",
    "#             '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1.ckpt',\n",
    "#             '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1-v1.ckpt',\n",
    "#             '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1-v2.ckpt',\n",
    "#             ######### 4-skf split \n",
    "#         ]\n",
    "#     }, \n",
    "    \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'gluon_senet154',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'threshold': 0.5, #0.24,\n",
    "#         'call_sign': \"roh_02\",\n",
    "#         'model_func': load_model_exp28_snapshot,        \n",
    "#         'model_pth': '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1.ckpt',\n",
    "#         'tta': False, #True\n",
    "#     }, \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'gluon_senet154',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'threshold': 0.5, #0.24,\n",
    "#         'call_sign': \"roh_03\",\n",
    "#         'model_func': load_model_exp28_snapshot,        \n",
    "#         'model_pth': '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1-v1.ckpt',\n",
    "#         'tta': False, #True\n",
    "#     }, \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'gluon_senet154',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'threshold': 0.5, #0.24,\n",
    "#         'call_sign': \"roh_04\",\n",
    "#         'model_func': load_model_exp28_snapshot,        \n",
    "#         'model_pth': '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1-v2.ckpt',\n",
    "#         'tta': False, #True\n",
    "#     }, \n",
    "    \n",
    "#         {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'gluon_senet154',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'threshold': 0.5, #0.24,\n",
    "#         'call_sign': \"ioa_02\",\n",
    "#         'model_func': load_model_exp28_snapshot,\n",
    "#         'tta': False, #True\n",
    "#         'normalize': True,\n",
    "#         'model_pth': [\n",
    "#             ######## full train.csv \n",
    "#             '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1.ckpt',\n",
    "#             '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1-v1.ckpt',\n",
    "#             '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1-v2.ckpt',\n",
    "#             ######### 4-skf split \n",
    "#             #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0.ckpt',\n",
    "#             #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v1.ckpt',\n",
    "#             #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v2.ckpt',\n",
    "#             #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v3.ckpt',\n",
    "#             #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v4.ckpt',\n",
    "#             #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v5.ckpt'\n",
    "#         ]\n",
    "#     }, \n",
    "    \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'gluon_senet154',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'threshold': 0.5, #0.24,\n",
    "#         'call_sign': \"ioa_02_tta\",\n",
    "#         'model_func': load_model_exp28_snapshot,\n",
    "#         'tta': True, #True\n",
    "#         'normalize': True,\n",
    "#         'model_pth': [\n",
    "#             ######## full train.csv \n",
    "#             '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1.ckpt',\n",
    "#             '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1-v1.ckpt',\n",
    "#             '/home/rohits/pv1/Contrail_Detection/output/ioannis/Unet-gluon_senet154_fold-1-v2.ckpt',\n",
    "#             ######### 4-skf split \n",
    "#             #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0.ckpt',\n",
    "#             #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v1.ckpt',\n",
    "#             #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v2.ckpt',\n",
    "#             #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v3.ckpt',\n",
    "#             #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v4.ckpt',\n",
    "#             #     '/kaggle/input/contrails-exp28b-senet154-512/Unet-gluon_senet154_fold0-v5.ckpt'\n",
    "#         ]\n",
    "#     }, \n",
    "    \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'efficientnet-b7',\n",
    "#         'img_size': [256, 256],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold0.pth',\n",
    "#         'threshold': 0.24, #0.24,\n",
    "#         'call_sign': \"roh_01_tta\",\n",
    "#         'model_func': load_modelr1,\n",
    "#         'tta': True\n",
    "#     }, \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'efficientnet-b7',\n",
    "#         'img_size': [256, 256],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold0.pth',\n",
    "#         'threshold': 0.24, #0.24,\n",
    "#         'call_sign': \"roh_01\",\n",
    "#         'model_func': load_modelr1,\n",
    "#         'tta': False\n",
    "#     }, \n",
    "    \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'cs3darknet_l',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/exp_01_s2/Unet/cs3darknet_l-512/checkpoint_dice_ctrl_fold0.pth',\n",
    "#         'threshold': 0.24, #0.24,\n",
    "#         'call_sign': \"roh_02_tta\",\n",
    "#         'model_func': load_modelr2,\n",
    "#         'tta': True\n",
    "#     }, \n",
    "    \n",
    "#         {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'cs3edgenet_x',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/exp_01_s2/Unet/cs3edgenet_x-512/checkpoint_dice_ctrl_fold0.pth',\n",
    "#         'threshold': 0.24, #0.24,\n",
    "#         'call_sign': \"roh_03_tta\",\n",
    "#         'model_func': load_modelr2,\n",
    "#         'tta': True\n",
    "#     }, \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'cs3sedarknet_l',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/exp_01_s2/Unet/cs3sedarknet_l-512/checkpoint_dice_ctrl_fold0.pth',\n",
    "#         'threshold': 0.24, #0.24,\n",
    "#         'call_sign': \"roh_04_tta\",\n",
    "#         'model_func': load_modelr2,\n",
    "#         'tta': True\n",
    "#     }, \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'ecaresnet101d',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/exp_01_s2/Unet/ecaresnet101d-512/checkpoint_dice_ctrl_fold0.pth',\n",
    "#         'threshold': 0.24, #0.24,\n",
    "#         'call_sign': \"roh_05_tta\",\n",
    "#         'model_func': load_modelr2,\n",
    "#         'tta': True\n",
    "#     }, \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'ecaresnet269d',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/exp_01_s2/Unet/ecaresnet269d-512/checkpoint_dice_ctrl_fold0.pth',\n",
    "#         'threshold': 0.24, #0.24,\n",
    "#         'call_sign': \"roh_06_tta\",\n",
    "#         'model_func': load_modelr2,\n",
    "#         'tta': True\n",
    "#     }, \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'fbnetv3_g',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/exp_01_s2/Unet/fbnetv3_g-512/checkpoint_dice_ctrl_fold0.pth',\n",
    "#         'threshold': 0.24, #0.24,\n",
    "#         'call_sign': \"roh_07_tta\",\n",
    "#         'model_func': load_modelr2,\n",
    "#         'tta': True\n",
    "#     }, \n",
    "    \n",
    "        \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'tf_mixnet_l',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/exp_01_s2/Unet/tf_mixnet_l-512/checkpoint_dice_ctrl_fold0.pth',\n",
    "#         'threshold': 0.24, #0.24,\n",
    "#         'call_sign': \"roh_08_tta\",\n",
    "#         'model_func': load_modelr2,\n",
    "#         'tta': True\n",
    "#     }, \n",
    "    \n",
    "    \n",
    "\n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'maxvit_small_tf_512',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/maxvitfold0mix_maxvit_small_tf_512_best_epochstage2oof685cv657-00.bin',\n",
    "#         'threshold': 0.62, #0.24,\n",
    "#         'call_sign': \"nir_01_tta\",\n",
    "#         'model_func': load_model4,\n",
    "#         'tta': True\n",
    "#     },\n",
    "    \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'eca_nfnet_l1',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/ecanfnetl1v2fold0_eca_nfnet_l1_best_epochstage2-00.bin',\n",
    "#         'threshold': 0.62, #0.24,\n",
    "#         'call_sign': \"nir_02_tta\",\n",
    "#         'model_func': load_model2,\n",
    "#         'tta': True\n",
    "#     },\n",
    "#     {\n",
    "#         'model_name': 'Unet++',\n",
    "#         'backbone': 'tf_efficientnet_b8',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/effb8modelv2fold0_tf_efficientnet_b8_best_epochstage2-00.bin',\n",
    "#         'threshold': 0.79,\n",
    "#         'call_sign': \"nir_03_tta\", \n",
    "#         'model_func': load_model3,\n",
    "#         'tta': True\n",
    "#     },\n",
    "#     {\n",
    "#         'model_name': 'Unet++',\n",
    "#         'backbone': 'tf_efficientnet_b7_ns',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/contrail_01/output/nirjhar/qishentrialv2fpn512_tf_efficientnet_b7_ns_best_epochcv650lb675-00.bin',\n",
    "#         'threshold': 0.62, #0.24,\n",
    "#         'call_sign': \"nir_04_tta\", \n",
    "#         'model_func': load_model1,\n",
    "#         'tta': True\n",
    "#     }, \n",
    "    \n",
    "    \n",
    "    \n",
    "#         {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'eca_nfnet_l2',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/ecanfnetl2v2fold0_eca_nfnet_l2_best_epochstage2-00.bin',\n",
    "#         'threshold': 0.62, #0.24,\n",
    "#         'call_sign': \"nir_05_tta\",\n",
    "#         'model_func': load_model2,\n",
    "#         'tta': True\n",
    "#     },\n",
    "#     {\n",
    "#     'model_name': 'Unet++',\n",
    "#         'backbone': 'densenetblur121d',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/ecanfnetl2v2fold0_densenetblur121d_best_epochstage2-00.bin',\n",
    "#         'threshold': 0.79,\n",
    "#         'call_sign': \"nir_06_tta\", \n",
    "#         'model_func': load_model2,\n",
    "#         'tta': True\n",
    "#     },\n",
    "    \n",
    "#     {\n",
    "#     'model_name': 'Unet++',\n",
    "#         'backbone': 'convnextv2_base',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/convnextfold0_convnextv2_base_best_epochstage2-00.bin',\n",
    "#         'threshold': 0.79,\n",
    "#         'call_sign': \"nir_07_tta\", \n",
    "#         'model_func': load_model2,\n",
    "#         'tta': True\n",
    "#     },\n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'maxvit_small_tf_512',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/maxvitfold0mix_maxvit_small_tf_512_best_epochstage2oof685cv657-00.bin',\n",
    "#         'threshold': 0.62, #0.24,\n",
    "#         'call_sign': \"nir_01\",\n",
    "#         'model_func': load_model4,\n",
    "#         'tta': False\n",
    "#     },\n",
    "    \n",
    "    \n",
    "#     {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'eca_nfnet_l1',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/ecanfnetl1v2fold0_eca_nfnet_l1_best_epochstage2-00.bin',\n",
    "#         'threshold': 0.62, #0.24,\n",
    "#         'call_sign': \"nir_02\",\n",
    "#         'model_func': load_model2,\n",
    "#         'tta': False\n",
    "#     },\n",
    "#     {\n",
    "#         'model_name': 'Unet++',\n",
    "#         'backbone': 'tf_efficientnet_b8',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/effb8modelv2fold0_tf_efficientnet_b8_best_epochstage2-00.bin',\n",
    "#         'threshold': 0.79,\n",
    "#         'call_sign': \"nir_03\", \n",
    "#         'model_func': load_model3,\n",
    "#         'tta': False\n",
    "#     },\n",
    "#     {\n",
    "#         'model_name': 'Unet++',\n",
    "#         'backbone': 'tf_efficientnet_b7_ns',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/contrail_01/output/nirjhar/qishentrialv2fpn512_tf_efficientnet_b7_ns_best_epochcv650lb675-00.bin',\n",
    "#         'threshold': 0.62, #0.24,\n",
    "#         'call_sign': \"nir_04\", \n",
    "#         'model_func': load_model1,\n",
    "#         'tta': False\n",
    "#     }, \n",
    "    \n",
    "#         {\n",
    "#         'model_name': 'Unet',\n",
    "#         'backbone': 'eca_nfnet_l2',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/ecanfnetl2v2fold0_eca_nfnet_l2_best_epochstage2-00.bin',\n",
    "#         'threshold': 0.62, #0.24,\n",
    "#         'call_sign': \"nir_05\",\n",
    "#         'model_func': load_model2,\n",
    "#         'tta': False\n",
    "#     },\n",
    "#     {\n",
    "#     'model_name': 'Unet++',\n",
    "#         'backbone': 'densenetblur121d',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/ecanfnetl2v2fold0_densenetblur121d_best_epochstage2-00.bin',\n",
    "#         'threshold': 0.79,\n",
    "#         'call_sign': \"nir_06\", \n",
    "#         'model_func': load_model2,\n",
    "#         'tta': False\n",
    "#     },\n",
    "    \n",
    "#     {\n",
    "#     'model_name': 'Unet++',\n",
    "#         'backbone': 'convnextv2_base',\n",
    "#         'img_size': [512, 512],\n",
    "#         'num_classes': 1,\n",
    "#         'model_pth':  '/home/rohits/pv1/Contrail_Detection/output/nirjhar/convnextfold0_convnextv2_base_best_epochstage2-00.bin',\n",
    "#         'threshold': 0.79,\n",
    "#         'call_sign': \"nir_07\", \n",
    "#         'model_func': load_model2,\n",
    "#         'tta': False\n",
    "#     },\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ed84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c09a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
