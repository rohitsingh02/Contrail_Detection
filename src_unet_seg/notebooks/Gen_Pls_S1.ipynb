{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c125d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ttach as tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645a79e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e5af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import cv2\n",
    "from torch.cuda import amp\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torchvision.utils import make_grid\n",
    "import optuna\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "157ff8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12e6bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dde45d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"1\"\n",
    "# NTB = 7\n",
    "\n",
    "folder = \"train\"  # \"train\"\n",
    "if folder == \"validation\":\n",
    "    target = \"1\"\n",
    "    \n",
    "NTB = 5 # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a7c03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fbb5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CFGS = [    \n",
    "    {\n",
    "        'model_name': 'Unet',\n",
    "        'backbone': 'timm-efficientnet-b8',\n",
    "        'img_size': [256, 256],\n",
    "        'num_classes': 1,\n",
    "        'model_pth':  '../../output/exp_s1_01/Unet/timm-efficientnet-b8/checkpoint_dice.pth',\n",
    "        'threshold': 0.3,\n",
    "        'tta': True\n",
    "    }, \n",
    "    \n",
    "    {\n",
    "    'model_name': 'Unet',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'img_size': [512, 512],\n",
    "        'num_classes': 1,\n",
    "        'model_pth':  '../../output/contrail_exp_b7/efficientnet-b7-512/checkpoint_dice.pth',\n",
    "        'threshold': 0.1,\n",
    "        'tta': True\n",
    "    }, \n",
    "    {\n",
    "    'model_name': 'Unet',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'img_size': [384, 384],\n",
    "        'num_classes': 1,\n",
    "        'model_pth':  '../../output/contrail_exp_b7/efficientnet-b7-384/checkpoint_dice.pth',\n",
    "        'threshold': 0.16,\n",
    "        'tta': True\n",
    "    }, \n",
    "    {\n",
    "    'model_name': 'Unet',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'img_size': [256, 256],\n",
    "        'num_classes': 1,\n",
    "        'model_pth':  '../../output/contrail_exp_b7/efficientnet-b7-256-100-epochs/checkpoint_dice.pth',\n",
    "        'threshold': 0.04,\n",
    "        'tta': True\n",
    "    },\n",
    "    {\n",
    "    'model_name': 'Unet',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'img_size': [256, 256],\n",
    "        'num_classes': 1,\n",
    "        'model_pth':  '../../output/contrail_exp_b7/efficientnet-b7-256-bce-train_2/checkpoint_dice.pth',\n",
    "        'threshold': 0.32,\n",
    "        'tta': False\n",
    "    },\n",
    "    {\n",
    "    'model_name': 'Unet',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'img_size': [256, 256],\n",
    "        'num_classes': 1,\n",
    "        'model_pth':  '../../output/contrail_exp_b7/efficientnet-b7-256-train_2/checkpoint_dice.pth',\n",
    "        'threshold': 0.32,\n",
    "        'tta': False\n",
    "    },\n",
    "    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ddaafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a095c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": A.Compose([\n",
    "        A.Resize(*CFGS[0]['img_size'], interpolation=cv2.INTER_NEAREST),\n",
    "        ], p=1.0),\n",
    "    \n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(*CFGS[0]['img_size'], interpolation=cv2.INTER_NEAREST),\n",
    "        ], p=1.0),\n",
    "    \n",
    "    \n",
    "    \"test\": A.Compose([\n",
    "            A.Resize(*CFGS[0]['img_size'], interpolation=cv2.INTER_NEAREST),\n",
    "    ], p=1.0),\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d58a5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrailDataset:\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df  \n",
    "        self.images = df['image']\n",
    "        self.transform =transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "#         print(\"../../input\" + self.images[idx])\n",
    "        image = np.load(\"../../input\" + self.images[idx]).astype(float)   \n",
    "        \n",
    "        if self.transform :\n",
    "            data = self.transform(image=image)\n",
    "            image  = data['image']\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        return torch.tensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "985b3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=cfg[\"backbone\"],      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "            encoder_weights=None,     # use `imagenet` pre-trained weights for encoder initialization\n",
    "            in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "            classes=cfg[\"num_classes\"],        # model output channels (number of classes in your dataset)\n",
    "            activation=None\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        mask = self.model(inputs)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07bfce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#  'w1': 0.951382694462493,\n",
    "#  'w2': 0.36024417206973613,\n",
    "#  'w3': 0.16498218172962048,\n",
    "#  'w4': 0.6047624543660389,\n",
    "#  'w5': 0.5106552868387371,\n",
    "#  'w6': 0.33710127631320935\n",
    "# }\n",
    "\n",
    "# params = {\n",
    "#  'w1': 0.33623293716110597,\n",
    "#  'w2': 0.9737207909271809,\n",
    "#  'w3': 0.3083112197058403,\n",
    "#  'w4': 0.4905850723996572,\n",
    "#  'w5': 0.19376624099524614,\n",
    "#  'w6': 0.7643271123289296,\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    'w1': 0.5294649933133848,\n",
    "    'w2': 0.9807394388336473,\n",
    "    'w3': 0.4183536450466668,\n",
    "    'w4': 0.13610731095623466,\n",
    "    'w5': 0.7324196781702746,\n",
    "    'w6': 0.6022020327386348\n",
    "}\n",
    "\n",
    "threshold = 0.14\n",
    "\n",
    "def weighted_ensemble(params, final_preds):    \n",
    "    for index, val in enumerate(params.keys()):\n",
    "        if index == 0:\n",
    "            preds = params[val]*final_preds[0]\n",
    "        else:\n",
    "            preds += params[val]*final_preds[index]\n",
    "    \n",
    "    param_sum = 0\n",
    "    for key, val in params.items():\n",
    "        param_sum += val\n",
    "\n",
    "    preds = preds/param_sum\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_transform(img_size):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(*img_size, interpolation=cv2.INTER_NEAREST),\n",
    "    ], p=1.0)\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f99c904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'timm-efficientnet-b8', 'img_size': [256, 256], 'num_classes': 1, 'model_pth': '../../output/exp_s1_01/Unet/timm-efficientnet-b8/checkpoint_dice.pth', 'threshold': 0.3, 'tta': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17eddacba5494efd9aab38fe0c5a9e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'efficientnet-b7', 'img_size': [512, 512], 'num_classes': 1, 'model_pth': '../../output/contrail_exp_b7/efficientnet-b7-512/checkpoint_dice.pth', 'threshold': 0.1, 'tta': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8a17178d6c4365a87e6feaecdefc7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'efficientnet-b7', 'img_size': [384, 384], 'num_classes': 1, 'model_pth': '../../output/contrail_exp_b7/efficientnet-b7-384/checkpoint_dice.pth', 'threshold': 0.16, 'tta': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10f5ee9aa474e6393cdec10e7c3733c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'efficientnet-b7', 'img_size': [256, 256], 'num_classes': 1, 'model_pth': '../../output/contrail_exp_b7/efficientnet-b7-256-100-epochs/checkpoint_dice.pth', 'threshold': 0.04, 'tta': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69c50069f8344049584ec4823d18817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'efficientnet-b7', 'img_size': [256, 256], 'num_classes': 1, 'model_pth': '../../output/contrail_exp_b7/efficientnet-b7-256-bce-train_2/checkpoint_dice.pth', 'threshold': 0.32, 'tta': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c71f8ed77cc47faa907bdb3f4609898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'efficientnet-b7', 'img_size': [256, 256], 'num_classes': 1, 'model_pth': '../../output/contrail_exp_b7/efficientnet-b7-256-train_2/checkpoint_dice.pth', 'threshold': 0.32, 'tta': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a552116e557b435c946f26f47b5fab51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/145 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if target == \"0\":\n",
    "    val_df = pd.read_csv(f'../../input/pseudo/{folder}_data_empty_{NTB}.csv') \n",
    "else:\n",
    "    val_df = pd.read_csv(f'../../input/pseudo/{folder}_data_{NTB}.csv') \n",
    "\n",
    "    \n",
    "final_preds = []\n",
    "\n",
    "for idx, cfg in enumerate(CFGS):    \n",
    "    val_df['id'] = val_df['label'].apply(lambda x: x.split('/')[3])\n",
    "\n",
    "    val_transform = data_transforms[\"test\"]\n",
    "    valid_dataset = ContrailDataset(val_df, transform=get_transform(cfg['img_size']))  \n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = 64, \n",
    "        shuffle = False, \n",
    "        num_workers = 2, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(cfg)\n",
    "#     model = Net(cfg['backbone'], cfg['num_classes'])\n",
    "    model = Net(cfg) \n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    \n",
    "    model.load_state_dict(torch.load(cfg['model_pth'], map_location=torch.device('cpu'))['model'])\n",
    "    if cfg['tta']:\n",
    "        model = tta.SegmentationTTAWrapper(model, tta.aliases.flip_transform(), merge_mode='mean')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for idx, image in enumerate(tqdm(valid_loader)):  \n",
    "        image = image.to(device, dtype=torch.float)    \n",
    "        with torch.inference_mode():\n",
    "            pred = model(image)\n",
    "            if cfg['img_size'][0] != 256:\n",
    "                pred = torch.nn.functional.interpolate(pred, size=256, mode='nearest') \n",
    "            preds.append(pred) \n",
    "    final_preds.append(torch.cat(preds))\n",
    "    \n",
    "    \n",
    "    del model, preds\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "final_preds = weighted_ensemble(params, final_preds)\n",
    "final_preds = (nn.Sigmoid()(final_preds)>threshold).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f79f6974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ae55270dfe4ef3a0ce6ffc4719e25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9259 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = val_df['id'].values\n",
    "for (val, label_id) in tqdm(zip(final_preds, ids), total=len(ids)): \n",
    "    mask = val.view(256, 256, 1).detach().cpu().numpy()\n",
    "    \n",
    "    if target == \"0\":\n",
    "        np.save(f\"../../input/pseudo/{folder}_data_empty_{NTB}/{label_id}/label.npy\", mask.astype('float16')) \n",
    "    else:\n",
    "        np.save(f\"../../input/pseudo/{folder}_data_{NTB}/{label_id}/label.npy\", mask.astype('float16')) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "643e7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df.image.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d04c7b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9259, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b25aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac38d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea84d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d845b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
