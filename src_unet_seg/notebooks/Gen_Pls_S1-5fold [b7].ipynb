{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c125d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ttach as tta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "645a79e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77e5af39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "# from tqdm.auto import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import cv2\n",
    "from torch.cuda import amp\n",
    "\n",
    "import torchvision.transforms as T\n",
    "from torchvision.transforms import Compose, ToTensor, Resize\n",
    "from torchvision.utils import make_grid\n",
    "import optuna\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "157ff8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e6bf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dde45d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"1\"\n",
    "# NTB = 7\n",
    "\n",
    "folder = \"train\"  # \"train\"\n",
    "if folder == \"validation\":\n",
    "    target = \"1\"\n",
    "    \n",
    "NTB = 5 # 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a7c03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fbb5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CFGS = [        \n",
    "    {\n",
    "        'model_name': 'Unet',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'img_size': [256, 256],\n",
    "        'num_classes': 1,\n",
    "        'model_pth': '../../output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold0.pth',\n",
    "        'threshold': 0.34,\n",
    "        'trained_by': 'rohit',\n",
    "        'tta': True, \n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Unet',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'img_size': [256, 256],\n",
    "        'num_classes': 1,\n",
    "        'model_pth': '../../output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold1.pth',\n",
    "        'threshold': 0.34,\n",
    "        'trained_by': 'rohit',\n",
    "        'tta': True, \n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Unet',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'img_size': [256, 256],\n",
    "        'num_classes': 1,\n",
    "        'model_pth': '../../output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold2.pth',\n",
    "        'threshold': 0.34,\n",
    "        'trained_by': 'rohit',\n",
    "        'tta': True, \n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Unet',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'img_size': [256, 256],\n",
    "        'num_classes': 1,\n",
    "        'model_pth': '../../output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold3.pth',\n",
    "        'threshold': 0.34,\n",
    "        'trained_by': 'rohit',\n",
    "        'tta': True, \n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Unet',\n",
    "        'backbone': 'efficientnet-b7',\n",
    "        'img_size': [256, 256],\n",
    "        'num_classes': 1,\n",
    "        'model_pth': '../../output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold4.pth',\n",
    "        'threshold': 0.34,\n",
    "        'trained_by': 'rohit',\n",
    "        'tta': True, \n",
    "    },\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ddaafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a095c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\": A.Compose([\n",
    "        A.Resize(*CFGS[0]['img_size'], interpolation=cv2.INTER_NEAREST),\n",
    "        ], p=1.0),\n",
    "    \n",
    "    \"valid\": A.Compose([\n",
    "        A.Resize(*CFGS[0]['img_size'], interpolation=cv2.INTER_NEAREST),\n",
    "        ], p=1.0),\n",
    "    \n",
    "    \n",
    "    \"test\": A.Compose([\n",
    "            A.Resize(*CFGS[0]['img_size'], interpolation=cv2.INTER_NEAREST),\n",
    "    ], p=1.0),\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d58a5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrailDataset:\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df  \n",
    "        self.images = df['image']\n",
    "        self.transform =transform\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "#         print(\"../../input\" + self.images[idx])\n",
    "        image = np.load(\"../../input\" + self.images[idx]).astype(float)   \n",
    "        \n",
    "        if self.transform :\n",
    "            data = self.transform(image=image)\n",
    "            image  = data['image']\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "        return torch.tensor(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "985b3de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.model = smp.Unet(\n",
    "            encoder_name=cfg[\"backbone\"],      # choose encoder, e.g. mobilenet_v2 or efficientnet-b7\n",
    "            encoder_weights=None,     # use `imagenet` pre-trained weights for encoder initialization\n",
    "            in_channels=3,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)\n",
    "            classes=cfg[\"num_classes\"],        # model output channels (number of classes in your dataset)\n",
    "            activation=None\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        mask = self.model(inputs)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07bfce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#  'w1': 0.951382694462493,\n",
    "#  'w2': 0.36024417206973613,\n",
    "#  'w3': 0.16498218172962048,\n",
    "#  'w4': 0.6047624543660389,\n",
    "#  'w5': 0.5106552868387371,\n",
    "#  'w6': 0.33710127631320935\n",
    "# }\n",
    "\n",
    "# params = {\n",
    "#  'w1': 0.33623293716110597,\n",
    "#  'w2': 0.9737207909271809,\n",
    "#  'w3': 0.3083112197058403,\n",
    "#  'w4': 0.4905850723996572,\n",
    "#  'w5': 0.19376624099524614,\n",
    "#  'w6': 0.7643271123289296,\n",
    "# }\n",
    "\n",
    "params = {\n",
    "    'w1': 0.5294649933133848,\n",
    "    'w2': 0.9807394388336473,\n",
    "    'w3': 0.4183536450466668,\n",
    "    'w4': 0.13610731095623466,\n",
    "    'w5': 0.7324196781702746,\n",
    "    'w6': 0.6022020327386348\n",
    "}\n",
    "\n",
    "threshold = 0.14\n",
    "\n",
    "def weighted_ensemble(params, final_preds):    \n",
    "    for index, val in enumerate(params.keys()):\n",
    "        if index == 0:\n",
    "            preds = params[val]*final_preds[0]\n",
    "        else:\n",
    "            preds += params[val]*final_preds[index]\n",
    "    \n",
    "    param_sum = 0\n",
    "    for key, val in params.items():\n",
    "        param_sum += val\n",
    "\n",
    "    preds = preds/param_sum\n",
    "    return preds\n",
    "\n",
    "\n",
    "def get_transform(img_size):\n",
    "    transform = A.Compose([\n",
    "        A.Resize(*img_size, interpolation=cv2.INTER_NEAREST),\n",
    "    ], p=1.0)\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f99c904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'efficientnet-b7', 'img_size': [256, 256], 'num_classes': 1, 'model_pth': '../../output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold0.pth', 'threshold': 0.34, 'trained_by': 'rohit', 'tta': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de130351f9854ed2b64ee1cb62944b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'efficientnet-b7', 'img_size': [256, 256], 'num_classes': 1, 'model_pth': '../../output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold1.pth', 'threshold': 0.34, 'trained_by': 'rohit', 'tta': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4401b6d06f814771921f122a82c82580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'efficientnet-b7', 'img_size': [256, 256], 'num_classes': 1, 'model_pth': '../../output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold2.pth', 'threshold': 0.34, 'trained_by': 'rohit', 'tta': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c1fdd51ab849beb6b86ff2d07fe4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'efficientnet-b7', 'img_size': [256, 256], 'num_classes': 1, 'model_pth': '../../output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold3.pth', 'threshold': 0.34, 'trained_by': 'rohit', 'tta': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8007d8dfaedd45b290e92d561b50406d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'Unet', 'backbone': 'efficientnet-b7', 'img_size': [256, 256], 'num_classes': 1, 'model_pth': '../../output/exp_01_s2/Unet/efficientnet-b7-256/checkpoint_dice_fold4.pth', 'threshold': 0.34, 'trained_by': 'rohit', 'tta': True}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38345365df1646f1a5576fd92f90500e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/321 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if target == \"0\":\n",
    "#     val_df = pd.read_csv(f'../../input/pseudo/{folder}_data_empty_{NTB}.csv') \n",
    "# else:\n",
    "val_df = pd.read_csv(f'../../input/pseudo/{folder}_data_{NTB}.csv') \n",
    "\n",
    "    \n",
    "final_preds = []\n",
    "\n",
    "for idx, cfg in enumerate(CFGS):    \n",
    "    val_df['id'] = val_df['label'].apply(lambda x: x.split('/')[3])\n",
    "\n",
    "    val_transform = data_transforms[\"test\"]\n",
    "    valid_dataset = ContrailDataset(val_df, transform=get_transform(cfg['img_size']))  \n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = 64, \n",
    "        shuffle = False, \n",
    "        num_workers = 2, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    \n",
    "    \n",
    "    print(cfg)\n",
    "#     model = Net(cfg['backbone'], cfg['num_classes'])\n",
    "    model = Net(cfg) \n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    \n",
    "    model.load_state_dict(torch.load(cfg['model_pth'], map_location=torch.device('cpu'))['model'])\n",
    "    if cfg['tta']:\n",
    "        model = tta.SegmentationTTAWrapper(model, tta.aliases.flip_transform(), merge_mode='mean')\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for idx, image in enumerate(tqdm(valid_loader)):  \n",
    "        image = image.to(device, dtype=torch.float)    \n",
    "        with torch.inference_mode():\n",
    "            pred = model(image)\n",
    "            pred = torch.nn.functional.interpolate(pred.sigmoid(), size=256, mode='nearest') \n",
    "            preds.append(pred) \n",
    "    final_preds.append(torch.cat(preds).detach().cpu())\n",
    "    \n",
    "    \n",
    "    del model, preds\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "# final_preds = weighted_ensemble(params, final_preds)\n",
    "\n",
    "final_preds = torch.stack(final_preds).mean(dim=0)\n",
    "\n",
    "threshold = 0.37\n",
    "final_preds = (final_preds>threshold).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f79f6974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827dc83718e646a595b9fcf9109b6deb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20529 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ids = val_df['id'].values\n",
    "for (val, label_id) in tqdm(zip(final_preds, ids), total=len(ids)): \n",
    "    mask = val.view(256, 256, 1).detach().cpu().numpy()\n",
    "    \n",
    "#     if target == \"0\":\n",
    "#         np.save(f\"../../input/pseudo/{folder}_data_empty_{NTB}/{label_id}/label_fold0_s1.npy\", mask.astype('float16')) \n",
    "#     else:\n",
    "    np.save(f\"../../input/pseudo/{folder}_data_{NTB}/{label_id}/label_5fold_s1.npy\", mask.astype('float16')) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "643e7d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_df.image.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d04c7b26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20529, 4)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35b25aeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/pseudo/train_data_5/1284412112608546821/image...</td>\n",
       "      <td>/pseudo/train_data_5/1284412112608546821/label...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1284412112608546821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/pseudo/train_data_5/7457695218848685981/image...</td>\n",
       "      <td>/pseudo/train_data_5/7457695218848685981/label...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7457695218848685981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/pseudo/train_data_5/836236084461732921/image.npy</td>\n",
       "      <td>/pseudo/train_data_5/836236084461732921/label.npy</td>\n",
       "      <td>1.0</td>\n",
       "      <td>836236084461732921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/pseudo/train_data_5/7829917977180135058/image...</td>\n",
       "      <td>/pseudo/train_data_5/7829917977180135058/label...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7829917977180135058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/pseudo/train_data_5/5319255125658459358/image...</td>\n",
       "      <td>/pseudo/train_data_5/5319255125658459358/label...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5319255125658459358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20524</th>\n",
       "      <td>/pseudo/train_data_5/8443915190215904823/image...</td>\n",
       "      <td>/pseudo/train_data_5/8443915190215904823/label...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8443915190215904823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20525</th>\n",
       "      <td>/pseudo/train_data_5/8495643844280686935/image...</td>\n",
       "      <td>/pseudo/train_data_5/8495643844280686935/label...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8495643844280686935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20526</th>\n",
       "      <td>/pseudo/train_data_5/856381910009426679/image.npy</td>\n",
       "      <td>/pseudo/train_data_5/856381910009426679/label.npy</td>\n",
       "      <td>2.0</td>\n",
       "      <td>856381910009426679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20527</th>\n",
       "      <td>/pseudo/train_data_5/3751790308836191485/image...</td>\n",
       "      <td>/pseudo/train_data_5/3751790308836191485/label...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3751790308836191485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20528</th>\n",
       "      <td>/pseudo/train_data_5/3682248240277515748/image...</td>\n",
       "      <td>/pseudo/train_data_5/3682248240277515748/label...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3682248240277515748</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20529 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   image  \\\n",
       "0      /pseudo/train_data_5/1284412112608546821/image...   \n",
       "1      /pseudo/train_data_5/7457695218848685981/image...   \n",
       "2      /pseudo/train_data_5/836236084461732921/image.npy   \n",
       "3      /pseudo/train_data_5/7829917977180135058/image...   \n",
       "4      /pseudo/train_data_5/5319255125658459358/image...   \n",
       "...                                                  ...   \n",
       "20524  /pseudo/train_data_5/8443915190215904823/image...   \n",
       "20525  /pseudo/train_data_5/8495643844280686935/image...   \n",
       "20526  /pseudo/train_data_5/856381910009426679/image.npy   \n",
       "20527  /pseudo/train_data_5/3751790308836191485/image...   \n",
       "20528  /pseudo/train_data_5/3682248240277515748/image...   \n",
       "\n",
       "                                                   label  fold  \\\n",
       "0      /pseudo/train_data_5/1284412112608546821/label...   4.0   \n",
       "1      /pseudo/train_data_5/7457695218848685981/label...   1.0   \n",
       "2      /pseudo/train_data_5/836236084461732921/label.npy   1.0   \n",
       "3      /pseudo/train_data_5/7829917977180135058/label...   4.0   \n",
       "4      /pseudo/train_data_5/5319255125658459358/label...   3.0   \n",
       "...                                                  ...   ...   \n",
       "20524  /pseudo/train_data_5/8443915190215904823/label...   3.0   \n",
       "20525  /pseudo/train_data_5/8495643844280686935/label...   2.0   \n",
       "20526  /pseudo/train_data_5/856381910009426679/label.npy   2.0   \n",
       "20527  /pseudo/train_data_5/3751790308836191485/label...   0.0   \n",
       "20528  /pseudo/train_data_5/3682248240277515748/label...   3.0   \n",
       "\n",
       "                        id  \n",
       "0      1284412112608546821  \n",
       "1      7457695218848685981  \n",
       "2       836236084461732921  \n",
       "3      7829917977180135058  \n",
       "4      5319255125658459358  \n",
       "...                    ...  \n",
       "20524  8443915190215904823  \n",
       "20525  8495643844280686935  \n",
       "20526   856381910009426679  \n",
       "20527  3751790308836191485  \n",
       "20528  3682248240277515748  \n",
       "\n",
       "[20529 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac38d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaea84d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d845b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
